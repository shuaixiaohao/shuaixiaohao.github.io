<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="mLX6JF6IDezTbDnbAdY2HZJGAwyNtqYritWrJQBoKTU" />








  <meta name="baidu-site-verification" content="5NEGr2UF8O" />
  <meta name="baidu-site-verification" content="awsaNG8J7E" />







  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/fh_icon32.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/fh_icon16.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="爬虫," />










<meta name="description" content="1、网络爬虫和相关工具1.1 网络爬虫网络爬虫（web crawler），以前经常称之为网络蜘蛛（spider），是按照一定的规则自动浏览万维网并获取信息的机器人程序（或脚本），曾经被广泛的应用于互联网搜索引擎。使用过互联网和浏览器的人都知道，网页中除了供用户阅读的文字信息之外，还包含一些超链接。网络爬虫系统正是通过网页中的超链接信息不断获得网络上的其它页面。正因如此，网络数据采集的过程就像一个爬">
<meta name="keywords" content="爬虫">
<meta property="og:type" content="article">
<meta property="og:title" content="爬虫">
<meta property="og:url" content="https://shuaixiaohao.github.io/2017/12/07/爬虫/index.html">
<meta property="og:site_name" content="帅小昊-博客">
<meta property="og:description" content="1、网络爬虫和相关工具1.1 网络爬虫网络爬虫（web crawler），以前经常称之为网络蜘蛛（spider），是按照一定的规则自动浏览万维网并获取信息的机器人程序（或脚本），曾经被广泛的应用于互联网搜索引擎。使用过互联网和浏览器的人都知道，网页中除了供用户阅读的文字信息之外，还包含一些超链接。网络爬虫系统正是通过网页中的超链接信息不断获得网络上的其它页面。正因如此，网络数据采集的过程就像一个爬">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/baidu-search-taobao.png">
<meta property="og:image" content="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/http-request.png">
<meta property="og:image" content="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/http-response.png">
<meta property="og:image" content="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/chrome-developer-tools.png">
<meta property="og:image" content="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/postman.png">
<meta property="og:image" content="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/crawler-workflow.png">
<meta property="og:image" content="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/lxml_whl.png">
<meta property="og:image" content="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/spider_01_useragent.png">
<meta property="og:image" content="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/threading_many_join.png">
<meta property="og:image" content="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/threading_many_deamon.png">
<meta property="og:image" content="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/threading_many_start.png">
<meta property="og:image" content="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/threading_many_run.png">
<meta property="og:image" content="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/yield_shenchengqi.png">
<meta property="og:image" content="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/spider_scrapy_zhujian.png">
<meta property="og:image" content="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/spider_scrapy_pip.png">
<meta property="og:image" content="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/scrapy_win32api.png">
<meta property="og:image" content="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/easyinstall_win32api.png">
<meta property="og:image" content="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/scrapy_run_not_error.png">
<meta property="og:image" content="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/spider_scrapy_project.png">
<meta property="og:image" content="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/scrapy_qidian_type.png">
<meta property="og:image" content="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/spider_scrapy_zhujian.png">
<meta property="og:image" content="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/scrapy_redis_tu.png">
<meta property="og:updated_time" content="2019-02-01T12:05:19.550Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="爬虫">
<meta name="twitter:description" content="1、网络爬虫和相关工具1.1 网络爬虫网络爬虫（web crawler），以前经常称之为网络蜘蛛（spider），是按照一定的规则自动浏览万维网并获取信息的机器人程序（或脚本），曾经被广泛的应用于互联网搜索引擎。使用过互联网和浏览器的人都知道，网页中除了供用户阅读的文字信息之外，还包含一些超链接。网络爬虫系统正是通过网页中的超链接信息不断获得网络上的其它页面。正因如此，网络数据采集的过程就像一个爬">
<meta name="twitter:image" content="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/baidu-search-taobao.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https'){
   bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
  }
  else{
  bp.src = 'http://push.zhanzhang.baidu.com/push.js';
  }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>





  <link rel="canonical" href="https://shuaixiaohao.github.io/2017/12/07/爬虫/"/>





  <title>爬虫 | 帅小昊-博客</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">帅小昊-博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">心有多大，舞台就有多大</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://shuaixiaohao.github.io/2017/12/07/爬虫/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="冯  昊">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/fenghao.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="帅小昊-博客">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">爬虫</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-12-07T22:09:32+08:00">
                2017-12-07
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/数据分析/" itemprop="url" rel="index">
                    <span itemprop="name">数据分析</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2017/12/07/爬虫/" class="leancloud_visitors" data-flag-title="爬虫">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="1、网络爬虫和相关工具"><a href="#1、网络爬虫和相关工具" class="headerlink" title="1、网络爬虫和相关工具"></a>1、网络爬虫和相关工具</h2><h3 id="1-1-网络爬虫"><a href="#1-1-网络爬虫" class="headerlink" title="1.1 网络爬虫"></a>1.1 网络爬虫</h3><p>网络爬虫（web crawler），以前经常称之为网络蜘蛛（spider），是按照一定的规则自动浏览万维网并获取信息的机器人程序（或脚本），曾经被广泛的应用于互联网搜索引擎。使用过互联网和浏览器的人都知道，网页中除了供用户阅读的文字信息之外，还包含一些超链接。网络爬虫系统正是通过网页中的超链接信息不断获得网络上的其它页面。正因如此，网络数据采集的过程就像一个爬虫或者蜘蛛在网络上漫游，所以才被形象的称为网络爬虫或者网络蜘蛛。</p>
<a id="more"></a> 
<h4 id="爬虫的应用领域"><a href="#爬虫的应用领域" class="headerlink" title="爬虫的应用领域"></a>爬虫的应用领域</h4><p>在理想的状态下，所有ICP（Internet Content Provider）都应该为自己的网站提供API接口来共享它们允许其他程序获取的数据，在这种情况下爬虫就不是必需品，国内比较有名的电商平台（如淘宝、京东等）、社交平台（如腾讯微博等）等网站都提供了自己的Open API，但是这类Open API通常会对可以抓取的数据以及抓取数据的频率进行限制。对于大多数的公司而言，及时的获取行业相关数据是企业生存的重要环节之一，然而大部分企业在行业数据方面的匮乏是其与生俱来的短板，合理的利用爬虫来获取数据并从中提取出有商业价值的信息是至关重要的。当然爬虫还有很多重要的应用领域，下面列举了其中的一部分：</p>
<ol>
<li>搜索引擎</li>
<li>新闻聚合</li>
<li>社交应用</li>
<li>舆情监控</li>
<li>行业数据</li>
</ol>
<h3 id="1-2合法性和背景调研"><a href="#1-2合法性和背景调研" class="headerlink" title="1.2合法性和背景调研"></a>1.2合法性和背景调研</h3><h4 id="爬虫合法性探讨"><a href="#爬虫合法性探讨" class="headerlink" title="爬虫合法性探讨"></a>爬虫合法性探讨</h4><ol>
<li>网络爬虫领域目前还属于拓荒阶段，虽然互联网世界已经通过自己的游戏规则建立起一定的道德规范(Robots协议，全称是“网络爬虫排除标准”)，但法律部分还在建立和完善中，也就是说，现在这个领域暂时还是灰色地带。</li>
<li>“法不禁止即为许可”，如果爬虫就像浏览器一样获取的是前端显示的数据（网页上的公开信息）而不是网站后台的私密敏感信息，就不太担心法律法规的约束，因为目前大数据产业链的发展速度远远超过了法律的完善程度。</li>
<li>在爬取网站的时候，需要限制自己的爬虫遵守Robots协议，同时控制网络爬虫程序的抓取数据的速度；在使用数据的时候，必须要尊重网站的知识产权（从Web 2.0时代开始，虽然Web上的数据很多都是由用户提供的，但是网站平台是投入了运营成本的，当用户在注册和发布内容时，平台通常就已经获得了对数据的所有权、使用权和分发权）。如果违反了这些规定，在打官司的时候败诉几率相当高。</li>
</ol>
<h4 id="Robots-txt文件"><a href="#Robots-txt文件" class="headerlink" title="Robots.txt文件"></a>Robots.txt文件</h4><p>大多数网站都会定义robots.txt文件，下面以淘宝的<a href="http://www.taobao.com/robots.txt" target="_blank" rel="noopener">robots.txt</a>文件为例，看看该网站对爬虫有哪些限制。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">User-agent:  Baiduspider</span><br><span class="line">Allow:  /article</span><br><span class="line">Allow:  /oshtml</span><br><span class="line">Disallow:  /product/</span><br><span class="line">Disallow:  /</span><br><span class="line"></span><br><span class="line">User-Agent:  Googlebot</span><br><span class="line">Allow:  /article</span><br><span class="line">Allow:  /oshtml</span><br><span class="line">Allow:  /product</span><br><span class="line">Allow:  /spu</span><br><span class="line">Allow:  /dianpu</span><br><span class="line">Allow:  /oversea</span><br><span class="line">Allow:  /list</span><br><span class="line">Disallow:  /</span><br><span class="line"></span><br><span class="line">User-agent:  Bingbot</span><br><span class="line">Allow:  /article</span><br><span class="line">Allow:  /oshtml</span><br><span class="line">Allow:  /product</span><br><span class="line">Allow:  /spu</span><br><span class="line">Allow:  /dianpu</span><br><span class="line">Allow:  /oversea</span><br><span class="line">Allow:  /list</span><br><span class="line">Disallow:  /</span><br><span class="line"></span><br><span class="line">User-Agent:  360Spider</span><br><span class="line">Allow:  /article</span><br><span class="line">Allow:  /oshtml</span><br><span class="line">Disallow:  /</span><br><span class="line"></span><br><span class="line">User-Agent:  Yisouspider</span><br><span class="line">Allow:  /article</span><br><span class="line">Allow:  /oshtml</span><br><span class="line">Disallow:  /</span><br><span class="line"></span><br><span class="line">User-Agent:  Sogouspider</span><br><span class="line">Allow:  /article</span><br><span class="line">Allow:  /oshtml</span><br><span class="line">Allow:  /product</span><br><span class="line">Disallow:  /</span><br><span class="line"></span><br><span class="line">User-Agent:  Yahoo!  Slurp</span><br><span class="line">Allow:  /product</span><br><span class="line">Allow:  /spu</span><br><span class="line">Allow:  /dianpu</span><br><span class="line">Allow:  /oversea</span><br><span class="line">Allow:  /list</span><br><span class="line">Disallow:  /</span><br><span class="line"></span><br><span class="line">User-Agent:  *</span><br><span class="line">Disallow:  /</span><br></pre></td></tr></table></figure>
<p>注意上面robots.txt第一段的最后一行，通过设置“Disallow: /”禁止百度爬虫访问除了“Allow”规定页面外的其他所有页面。因此当你在百度搜索“淘宝”的时候，搜索结果下方会出现：“由于该网站的robots.txt文件存在限制指令（限制搜索引擎抓取），系统无法提供该页面的内容描述”。百度作为一个搜索引擎，至少在表面上遵守了淘宝网的robots.txt协议，所以用户不能从百度上搜索到淘宝内部的产品信息。</p>
<p><img src="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/baidu-search-taobao.png" alt=""></p>
<h3 id="1-3-相关工具介绍"><a href="#1-3-相关工具介绍" class="headerlink" title="1.3 相关工具介绍"></a>1.3 相关工具介绍</h3><h4 id="HTTP协议"><a href="#HTTP协议" class="headerlink" title="HTTP协议"></a>HTTP协议</h4><p>在开始讲解爬虫之前，我们稍微对HTTP（超文本传输协议）做一些回顾，因为我们在网页上看到的内容通常是浏览器执行HTML语言得到的结果，而HTTP就是传输HTML数据的协议。HTTP和其他很多应用级协议一样是构建在TCP（传输控制协议）之上的，它利用了TCP提供的可靠的传输服务实现了Web应用中的数据交换。按照维基百科上的介绍，设计HTTP最初的目的是为了提供一种发布和接收<a href="https://zh.wikipedia.org/wiki/HTML" target="_blank" rel="noopener">HTML</a>页面的方法，也就是说这个协议是浏览器和Web服务器之间传输的数据的载体。关于这个协议的详细信息以及目前的发展状况，大家可以阅读阮一峰老师的<a href="http://www.ruanyifeng.com/blog/2016/08/http.html" target="_blank" rel="noopener">《HTTP 协议入门》</a>、<a href="http://www.ruanyifeng.com/blog/2012/05/internet_protocol_suite_part_i.html" target="_blank" rel="noopener">《互联网协议入门》</a>系列以及<a href="http://www.ruanyifeng.com/blog/2014/09/illustration-ssl.html" target="_blank" rel="noopener">《图解HTTPS协议》</a>进行了解，下图是我在四川省网络通信技术重点实验室工作期间用开源协议分析工具Ethereal（抓包工具WireShark的前身）截取的访问百度首页时的HTTP请求和响应的报文（协议数据），由于Ethereal截取的是经过网络适配器的数据，因此可以清晰的看到从物理链路层到应用层的协议数据。</p>
<p>HTTP请求（请求行+请求头+空行+[消息体]）：</p>
<p><img src="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/http-request.png" alt=""></p>
<p>HTTP响应（响应行+响应头+空行+消息体）：</p>
<p><img src="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/http-response.png" alt=""></p>
<blockquote>
<p>说明：但愿这两张如同泛黄的照片般的截图帮助你大概的了解到HTTP是一个怎样的协议。</p>
</blockquote>
<h4 id="相关工具"><a href="#相关工具" class="headerlink" title="相关工具"></a>相关工具</h4><ol>
<li><p>Chrome Developer Tools：谷歌浏览器内置的开发者工具。</p>
<p><img src="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/chrome-developer-tools.png" alt=""></p>
</li>
<li><p>POSTMAN：功能强大的网页调试与RESTful请求工具。</p>
<p><img src="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/postman.png" alt=""></p>
</li>
<li><p>HTTPie：命令行HTTP客户端。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ http --header http://www.scu.edu.cn</span><br><span class="line">HTTP/1.1 200 OK</span><br><span class="line">Accept-Ranges: bytes</span><br><span class="line">Cache-Control: private, max-age=600</span><br><span class="line">Connection: Keep-Alive</span><br><span class="line">Content-Encoding: gzip</span><br><span class="line">Content-Language: zh-CN</span><br><span class="line">Content-Length: 14403</span><br><span class="line">Content-Type: text/html</span><br><span class="line">Date: Sun, 27 May 2018 15:38:25 GMT</span><br><span class="line">ETag: &quot;e6ec-56d3032d70a32-gzip&quot;</span><br><span class="line">Expires: Sun, 27 May 2018 15:48:25 GMT</span><br><span class="line">Keep-Alive: timeout=5, max=100</span><br><span class="line">Last-Modified: Sun, 27 May 2018 13:44:22 GMT</span><br><span class="line">Server: VWebServer</span><br><span class="line">Vary: User-Agent,Accept-Encoding</span><br><span class="line">X-Frame-Options: SAMEORIGIN</span><br></pre></td></tr></table></figure>
</li>
<li><p>BuiltWith：识别网站所用技术的工具。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;</span><br><span class="line">&gt;&gt;&gt; import builtwith</span><br><span class="line">&gt;&gt;&gt; builtwith.parse(&apos;http://www.bootcss.com/&apos;)</span><br><span class="line">&#123;&apos;web-servers&apos;: [&apos;Nginx&apos;], &apos;font-scripts&apos;: [&apos;Font Awesome&apos;], &apos;javascript-frameworks&apos;: [&apos;Lo-dash&apos;, &apos;Underscore.js&apos;, &apos;Vue.js&apos;, &apos;Zepto&apos;, &apos;jQuery&apos;], &apos;web-frameworks&apos;: [&apos;Twitter Bootstrap&apos;]&#125;</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&gt;&gt;&gt; import ssl</span><br><span class="line">&gt;&gt;&gt; ssl._create_default_https_context = ssl._create_unverified_context</span><br><span class="line">&gt;&gt;&gt; builtwith.parse(&apos;https://www.jianshu.com/&apos;)</span><br><span class="line">&#123;&apos;web-servers&apos;: [&apos;Tengine&apos;], &apos;web-frameworks&apos;: [&apos;Twitter Bootstrap&apos;, &apos;Ruby on Rails&apos;], &apos;programming-languages&apos;: [&apos;Ruby&apos;]&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>python-whois：查询网站所有者的工具。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;</span><br><span class="line">&gt;&gt;&gt; import whois</span><br><span class="line">&gt;&gt;&gt; whois.whois(&apos;baidu.com&apos;)</span><br><span class="line">&#123;&apos;domain_name&apos;: [&apos;BAIDU.COM&apos;, &apos;baidu.com&apos;], &apos;registrar&apos;: &apos;MarkMonitor, Inc.&apos;, &apos;whois_server&apos;: &apos;whois.markmonitor.com&apos;, &apos;referral_url&apos;: None, &apos;updated_date&apos;: [datetime.datetime(2017, 7, 28, 2, 36, 28), datetime.datetime(2017, 7, 27, 19, 36, 28)], &apos;creation_date&apos;: [datetime.datetime(1999, 10, 11, 11, 5, 17), datetime.datetime(1999, 10, 11, 4, 5, 17)], &apos;expiration_date&apos;: [datetime.datetime(2026, 10, 11, 11, 5, 17), datetime.datetime(2026, 10, 11, 0, 0)], &apos;name_servers&apos;: [&apos;DNS.BAIDU.COM&apos;, &apos;NS2.BAIDU.COM&apos;, &apos;NS3.BAIDU.COM&apos;, &apos;NS4.BAIDU.COM&apos;, &apos;NS7.BAIDU.COM&apos;, &apos;dns.baidu.com&apos;, &apos;ns4.baidu.com&apos;, &apos;ns3.baidu.com&apos;, &apos;ns7.baidu.com&apos;, &apos;ns2.baidu.com&apos;], &apos;status&apos;: [&apos;clientDeleteProhibited https://icann.org/epp#clientDeleteProhibited&apos;, &apos;clientTransferProhibited https://icann.org/epp#clientTransferProhibited&apos;, &apos;clientUpdateProhibited https://icann.org/epp#clientUpdateProhibited&apos;, &apos;serverDeleteProhibited https://icann.org/epp#serverDeleteProhibited&apos;, &apos;serverTransferProhibited https://icann.org/epp#serverTransferProhibited&apos;, &apos;serverUpdateProhibited https://icann.org/epp#serverUpdateProhibited&apos;, &apos;clientUpdateProhibited (https://www.icann.org/epp#clientUpdateProhibited)&apos;, &apos;clientTransferProhibited (https://www.icann.org/epp#clientTransferProhibited)&apos;, &apos;clientDeleteProhibited (https://www.icann.org/epp#clientDeleteProhibited)&apos;, &apos;serverUpdateProhibited (https://www.icann.org/epp#serverUpdateProhibited)&apos;, &apos;serverTransferProhibited (https://www.icann.org/epp#serverTransferProhibited)&apos;, &apos;serverDeleteProhibited (https://www.icann.org/epp#serverDeleteProhibited)&apos;], &apos;emails&apos;: [&apos;abusecomplaints@markmonitor.com&apos;, &apos;whoisrelay@markmonitor.com&apos;], &apos;dnssec&apos;: &apos;unsigned&apos;, &apos;name&apos;: None, &apos;org&apos;: &apos;Beijing Baidu Netcom Science Technology Co., Ltd.&apos;, &apos;address&apos;: None, &apos;city&apos;: None, &apos;state&apos;: &apos;Beijing&apos;, &apos;zipcode&apos;: None, &apos;country&apos;: &apos;CN&apos;&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>robotparser：解析robots.txt的工具。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from urllib import robotparser</span><br><span class="line">&gt;&gt;&gt; parser = robotparser.RobotFileParser()</span><br><span class="line">&gt;&gt;&gt; parser.set_url(&apos;https://www.taobao.com/robots.txt&apos;)</span><br><span class="line">&gt;&gt;&gt; parser.read()</span><br><span class="line">&gt;&gt;&gt; parser.can_fetch(&apos;Hellokitty&apos;, &apos;http://www.taobao.com/article&apos;)</span><br><span class="line">False</span><br><span class="line">&gt;&gt;&gt; parser.can_fetch(&apos;Baiduspider&apos;, &apos;http://www.taobao.com/article&apos;)</span><br><span class="line">True</span><br><span class="line">&gt;&gt;&gt; parser.can_fetch(&apos;Baiduspider&apos;, &apos;http://www.taobao.com/product&apos;)</span><br><span class="line">False</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="一个简单的爬虫"><a href="#一个简单的爬虫" class="headerlink" title="一个简单的爬虫"></a>一个简单的爬虫</h3><p>一个基本的爬虫通常分为数据采集（网页下载）、数据处理（网页解析）和数据存储（将有用的信息持久化）三个部分的内容，当然更为高级的爬虫在数据采集和处理时会使用并发编程或分布式技术，这就需要有调度器（安排线程或进程执行对应的任务）、后台管理程序（监控爬虫的工作状态以及检查数据抓取的结果）等的参与。</p>
<p><img src="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/crawler-workflow.png" alt=""></p>
<p>一般来说，爬虫的工作流程包括以下几个步骤：</p>
<ol>
<li>设定抓取目标（种子页面/起始页面）并获取网页。</li>
<li>当服务器无法访问时，按照指定的重试次数尝试重新下载页面。</li>
<li>在需要的时候设置用户代理或隐藏真实IP，否则可能无法访问页面。</li>
<li>对获取的页面进行必要的解码操作然后抓取出需要的信息。</li>
<li>在获取的页面中通过某种方式（如正则表达式）抽取出页面中的链接信息。</li>
<li>对链接进行进一步的处理（获取页面并重复上面的动作）。</li>
<li>将有用的信息进行持久化以备后续的处理。</li>
</ol>
<p>下面的例子给出了一个从“搜狐体育”上获取NBA新闻标题和链接的爬虫。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">from urllib.error import URLError</span><br><span class="line">from urllib.request import urlopen</span><br><span class="line"></span><br><span class="line">import re</span><br><span class="line">import pymysql</span><br><span class="line">import ssl</span><br><span class="line"></span><br><span class="line">from pymysql import Error</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 通过指定的字符集对页面进行解码(不是每个网站都将字符集设置为utf-8)</span><br><span class="line">def decode_page(page_bytes, charsets=(&apos;utf-8&apos;,)):</span><br><span class="line">    page_html = None</span><br><span class="line">    for charset in charsets:</span><br><span class="line">        try:</span><br><span class="line">            page_html = page_bytes.decode(charset)</span><br><span class="line">            break</span><br><span class="line">        except UnicodeDecodeError:</span><br><span class="line">            pass</span><br><span class="line">            # logging.error(&apos;Decode:&apos;, error)</span><br><span class="line">    return page_html</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 获取页面的HTML代码(通过递归实现指定次数的重试操作)</span><br><span class="line">def get_page_html(seed_url, *, retry_times=3, charsets=(&apos;utf-8&apos;,)):</span><br><span class="line">    page_html = None</span><br><span class="line">    try:</span><br><span class="line">        page_html = decode_page(urlopen(seed_url).read(), charsets)</span><br><span class="line">    except URLError:</span><br><span class="line">        # logging.error(&apos;URL:&apos;, error)</span><br><span class="line">        if retry_times &gt; 0:</span><br><span class="line">            return get_page_html(seed_url, retry_times=retry_times - 1,</span><br><span class="line">                                 charsets=charsets)</span><br><span class="line">    return page_html</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 从页面中提取需要的部分(通常是链接也可以通过正则表达式进行指定)</span><br><span class="line">def get_matched_parts(page_html, pattern_str, pattern_ignore_case=re.I):</span><br><span class="line">    pattern_regex = re.compile(pattern_str, pattern_ignore_case)</span><br><span class="line">    return pattern_regex.findall(page_html) if page_html else []</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 开始执行爬虫程序并对指定的数据进行持久化操作</span><br><span class="line">def start_crawl(seed_url, match_pattern, *, max_depth=-1):</span><br><span class="line">    conn = pymysql.connect(host=&apos;localhost&apos;, port=3306,</span><br><span class="line">                           database=&apos;crawler&apos;, user=&apos;root&apos;,</span><br><span class="line">                           password=&apos;123456&apos;, charset=&apos;utf8&apos;)</span><br><span class="line">    try:</span><br><span class="line">        with conn.cursor() as cursor:</span><br><span class="line">            url_list = [seed_url]</span><br><span class="line">            # 通过下面的字典避免重复抓取并控制抓取深度</span><br><span class="line">            visited_url_list = &#123;seed_url: 0&#125;</span><br><span class="line">            while url_list:</span><br><span class="line">                current_url = url_list.pop(0)</span><br><span class="line">                depth = visited_url_list[current_url]</span><br><span class="line">                if depth != max_depth:</span><br><span class="line">                    # 尝试用utf-8/gbk/gb2312三种字符集进行页面解码</span><br><span class="line">                    page_html = get_page_html(current_url, charsets=(&apos;utf-8&apos;, &apos;gbk&apos;, &apos;gb2312&apos;))</span><br><span class="line">                    links_list = get_matched_parts(page_html, match_pattern)</span><br><span class="line">                    param_list = []</span><br><span class="line">                    for link in links_list:</span><br><span class="line">                        if link not in visited_url_list:</span><br><span class="line">                            visited_url_list[link] = depth + 1</span><br><span class="line">                            page_html = get_page_html(link, charsets=(&apos;utf-8&apos;, &apos;gbk&apos;, &apos;gb2312&apos;))</span><br><span class="line">                            headings = get_matched_parts(page_html, r&apos;&lt;h1&gt;(.*)&lt;span&apos;)</span><br><span class="line">                            if headings:</span><br><span class="line">                                param_list.append((headings[0], link))</span><br><span class="line">                    cursor.executemany(&apos;insert into tb_result values (default, %s, %s)&apos;,</span><br><span class="line">                                       param_list)</span><br><span class="line">                    conn.commit()</span><br><span class="line">    except Error:</span><br><span class="line">        pass</span><br><span class="line">        # logging.error(&apos;SQL:&apos;, error)</span><br><span class="line">    finally:</span><br><span class="line">        conn.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    ssl._create_default_https_context = ssl._create_unverified_context</span><br><span class="line">    start_crawl(&apos;http://sports.sohu.com/nba_a.shtml&apos;,</span><br><span class="line">                r&apos;&lt;a[^&gt;]+test=a\s[^&gt;]*href=[&quot;\&apos;](.*?)[&quot;\&apos;]&apos;,</span><br><span class="line">                max_depth=2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p>由于使用了MySQL实现持久化操作，所以要先启动MySQL服务器再运行该程序。</p>
<h3 id="爬虫注意事项"><a href="#爬虫注意事项" class="headerlink" title="爬虫注意事项"></a>爬虫注意事项</h3><p>通过上面的例子，我们对爬虫已经有了一个感性的认识，在编写爬虫时有以下一些注意事项：</p>
<ol>
<li><p>处理相对链接。有的时候我们从页面中获取的链接不是一个完整的绝对链接而是一个相对链接，这种情况下需要将其与URL前缀进行拼接（<code>urllib.parse</code>中的<code>urljoin()</code>函数可以完成此项操作）。</p>
</li>
<li><p>设置代理服务。有些网站会限制访问的区域（例如美国的Netflix屏蔽了很多国家的访问），有些爬虫需要隐藏自己的身份，在这种情况下可以设置使用代理服务器，代理服务器有免费（如<a href="http://www.xicidaili.com/" target="_blank" rel="noopener">西刺代理</a>、<a href="https://www.kuaidaili.com/free/" target="_blank" rel="noopener">快代理</a>）和付费两种（如<a href="http://www.xdaili.cn/" target="_blank" rel="noopener">讯代理</a>、<a href="https://www.abuyun.com/" target="_blank" rel="noopener">阿布云代理</a>)，付费的一般稳定性和可用性都更好，可以通过<code>urllib.request</code>中的<code>ProxyHandler</code>来为请求设置代理。</p>
</li>
<li><p>限制下载速度。如果我们的爬虫获取网页的速度过快，可能就会面临被封禁或者产生“损害动产”的风险（这个可能会导致吃官司且败诉），可以在两次下载之间添加延时从而对爬虫进行限速。</p>
</li>
<li><p>避免爬虫陷阱。有些网站会动态生成页面内容，这会导致产生无限多的页面（例如在线万年历通常会有无穷无尽的链接）。可以通过记录到达当前页面经过了多少个链接（链接深度）来解决该问题，当达到事先设定的最大深度时爬虫就不再像队列中添加该网页中的链接了。</p>
</li>
<li><p>SSL相关问题。在使用<code>urlopen</code>打开一个HTTPS链接时会验证一次SSL证书，如果不做出处理会产生错误提示“SSL: CERTIFICATE_VERIFY_FAILED”，可以通过以下两种方式加以解决：</p>
<ul>
<li><p>使用未经验证的上下文</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import ssl</span><br><span class="line"></span><br><span class="line">request = urllib.request.Request(url=&apos;...&apos;, headers=&#123;...&#125;) </span><br><span class="line">context = ssl._create_unverified_context()</span><br><span class="line">web_page = urllib.request.urlopen(request, context=context)</span><br></pre></td></tr></table></figure>
</li>
<li><p>设置全局的取消证书验证</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import ssl</span><br><span class="line"></span><br><span class="line">ssl._create_default_https_context = ssl._create_unverified_context</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ol>
<h2 id="2、前言"><a href="#2、前言" class="headerlink" title="2、前言"></a>2、前言</h2><h3 id="2-1-数据分析"><a href="#2-1-数据分析" class="headerlink" title="2.1 数据分析"></a>2.1 数据分析</h3><p>爬取网页信息可以使用很多的技术：</p>
<ol>
<li><p>获取网页信息：urllib、urllib3、requests</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">requests为第三方的库，需要安装才能使用</span><br><span class="line">   </span><br><span class="line">pip install requests</span><br></pre></td></tr></table></figure>
</li>
<li><p>解析网页信息：beautifulsoup4(bs4)、re、xpath、lxml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bs4为第三方的库，需要安装才能使用</span><br><span class="line">   </span><br><span class="line">pip install beautifulsoup4</span><br><span class="line">   </span><br><span class="line">使用的时候 from bs4 import BeautifulSoup 这样导入</span><br></pre></td></tr></table></figure>
<p>Python 标准库中自带了 xml 模块，但是性能不够好，而且缺乏一些人性化的 API，相比之下，第三方库 lxml 是用 Cython 实现的，而且增加了很多实用的功能。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">安装lxml，在新版本中无法使用from lxml import etree</span><br><span class="line"></span><br><span class="line">pip install lxml 并不推荐这样去安装lxml</span><br><span class="line">   </span><br><span class="line">推荐安装的方法：访问网站(https://www.lfd.uci.edu/~gohlke/pythonlibs/#lxml)下载lxml的安装whl文件，然后进行安装。</span><br></pre></td></tr></table></figure>
<p><strong>注意：下载文件必须与python版本号、位数一致</strong></p>
</li>
</ol>
<p>我这儿下载的是lxml-4.2.1-cp36-cp36m-win_amd64.whl，安装命令如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install lxml-4.2.1-cp36-cp36m-win_amd64.whl</span><br></pre></td></tr></table></figure>
<p>截图：</p>
<p><img src="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/lxml_whl.png" alt=""></p>
<ol>
<li><p>动态数据解析</p>
<p>通用：selenium(自动化测试框架)</p>
</li>
</ol>
<h3 id="2-2-请求头分析"><a href="#2-2-请求头分析" class="headerlink" title="2.2 请求头分析"></a>2.2 请求头分析</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># 浏览器告诉服务器可以接收的文本类型, */*表示任何类型都可以接收</span><br><span class="line">Accept: text/html, */*;q=0.8</span><br><span class="line"></span><br><span class="line"># 浏览器告诉服务器，数据可以压缩，页面可以解压数据然后进行渲染。做爬虫的时候，最好不要写该参数</span><br><span class="line">Accept-Encoding: gzip, deflate </span><br><span class="line"></span><br><span class="line"># 语言类型</span><br><span class="line">Accept-Language: zh-CN,zh;q=0.9 </span><br><span class="line"></span><br><span class="line">Cache-Control: max-age=0</span><br><span class="line"></span><br><span class="line"># 保持连接</span><br><span class="line">Connection: keep-alive </span><br><span class="line"></span><br><span class="line"># 会话 </span><br><span class="line">Cookie: Hm_lvt_3bfcc098e0da26d58c321ba579b04b2f=1527581188,1528137133</span><br><span class="line"></span><br><span class="line"># 域名</span><br><span class="line">Host: www.cdtopspeed.com </span><br><span class="line"></span><br><span class="line">Upgrade-Insecure-Requests: 1</span><br><span class="line"></span><br><span class="line"># 用户代理, 使得服务器能够识别请求是通过浏览器请求过来的，其中包含浏览器的名称/版本等信息</span><br><span class="line">User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36</span><br></pre></td></tr></table></figure>
<p>其中在爬虫中最重要的就是User-Agent：在下面urllib的使用中就会详细的解释User-Agent的使用</p>
<h3 id="2-3-urllib的使用"><a href="#2-3-urllib的使用" class="headerlink" title="2.3 urllib的使用"></a>2.3 urllib的使用</h3><p>使用urllib来获取百度首页的源码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">r = urllib.request.urlopen(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line">print(r.read().decode(<span class="string">'utf-8'</span>))</span><br></pre></td></tr></table></figure>
<p>按照我们的想法来说，输出的结果应该是百度首页的源码才对，但是输出却不对(多请求几次，就会出现如下的结果)，如下结果：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">	&lt;script&gt;</span><br><span class="line">		location.replace(location.href.replace(<span class="string">"https://"</span>,<span class="string">"http://"</span>));</span><br><span class="line">	&lt;/script&gt;</span><br><span class="line">&lt;/head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">	&lt;noscript&gt;&lt;meta http-equiv="refresh" content="0;url=http://www.baidu.com/"&gt;&lt;/noscript&gt;</span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure>
<p>以上的结果并不是我们想要的，我们可以来查看一下为什么会出现这种问题的原因。我们可以想到刚才说的，请求头中的最重要的参数User-Agent参数，该参数是用来告诉服务器，请求的url是来源于哪儿的，是来源于浏览器还是来源于其他地方的。如果是来源于非浏览器的会就不会返回源码信息给你的，直接拦截掉你的请求</p>
<p>分析以上代码中，默认提交的请求头中的User-Agent到底传递了什么值：</p>
<p><img src="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/spider_01_useragent.png" alt=""></p>
<p>接下来，就是优化以上的代码，实现目的就是告诉服务器我们这个请求是来源于浏览器的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">header = &#123;<span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko)Chrome/65.0.3325.181 Safari/537.36'</span>&#125;</span><br><span class="line"></span><br><span class="line">res = urllib.request.Request(<span class="string">'https://www.baidu.com'</span>, headers=header)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取url的页面源码</span></span><br><span class="line">r = urllib.request.urlopen(res)</span><br><span class="line"><span class="comment"># decode解码，encode编码</span></span><br><span class="line">print(r.read().decode(<span class="string">'utf-8'</span>))</span><br></pre></td></tr></table></figure>
<p>按照这样去解析，就可以获取到百度的首页源代码了，展示部门代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line"></span><br><span class="line">&lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html;charset=utf-8&quot;&gt;</span><br><span class="line">&lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=Edge&quot;&gt;</span><br><span class="line">&lt;meta content=&quot;always&quot; name=&quot;referrer&quot;&gt;</span><br><span class="line">&lt;meta name=&quot;theme-color&quot; content=&quot;#2932e1&quot;&gt;</span><br><span class="line">&lt;link rel=&quot;shortcut icon&quot; href=&quot;/favicon.ico&quot; type=&quot;image/x-icon&quot; /&gt;</span><br><span class="line">&lt;link rel=&quot;search&quot; type=&quot;application/opensearchdescription+xml&quot; href=&quot;/content-search.xml&quot; title=&quot;百度搜索&quot; /&gt;</span><br><span class="line">&lt;link rel=&quot;icon&quot; sizes=&quot;any&quot; mask href=&quot;//www.baidu.com/img/baidu_85beaf5496f291521eb75ba38eacbd87.svg&quot;&gt;</span><br><span class="line"></span><br><span class="line">&lt;link rel=&quot;dns-prefetch&quot; href=&quot;//s1.bdstatic.com&quot;/&gt;</span><br><span class="line">&lt;link rel=&quot;dns-prefetch&quot; href=&quot;//t1.baidu.com&quot;/&gt;</span><br><span class="line">&lt;link rel=&quot;dns-prefetch&quot; href=&quot;//t2.baidu.com&quot;/&gt;</span><br><span class="line">&lt;link rel=&quot;dns-prefetch&quot; href=&quot;//t3.baidu.com&quot;/&gt;</span><br><span class="line">&lt;link rel=&quot;dns-prefetch&quot; href=&quot;//t10.baidu.com&quot;/&gt;</span><br><span class="line">&lt;link rel=&quot;dns-prefetch&quot; href=&quot;//t11.baidu.com&quot;/&gt;</span><br><span class="line">&lt;link rel=&quot;dns-prefetch&quot; href=&quot;//t12.baidu.com&quot;/&gt;</span><br><span class="line">&lt;link rel=&quot;dns-prefetch&quot; href=&quot;//b1.bdstatic.com&quot;/&gt;</span><br><span class="line"></span><br><span class="line">&lt;title&gt;百度一下，你就知道&lt;/title&gt;</span><br><span class="line"></span><br><span class="line">&lt;style id=&quot;css_index&quot; index=&quot;index&quot; type=&quot;text/css&quot;&gt;html,body&#123;height:100%&#125;</span><br><span class="line">html&#123;overflow-y:auto&#125;</span><br><span class="line">body&#123;font:12px arial;text-align:;background:#fff&#125;</span><br><span class="line">body,p,form,ul,li&#123;margin:0;padding:0;list-style:none&#125;</span><br><span class="line">body,form,#fm&#123;position:relative&#125;</span><br><span class="line">td&#123;text-align:left&#125;</span><br><span class="line">img&#123;border:0&#125;</span><br><span class="line">a&#123;color:#00c&#125;</span><br><span class="line">a:active&#123;color:#f60&#125;</span><br><span class="line">input&#123;border:0;padding:0&#125;</span><br><span class="line">#wrapper&#123;position:relative;_position:;min-height:100%&#125;</span><br><span class="line">#head&#123;padding-bottom:100px;text-align:center;*z-index:1&#125;</span><br><span class="line"></span><br><span class="line">...忽略....</span><br><span class="line">...忽略....</span><br><span class="line">...忽略....</span><br><span class="line"></span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure>
<h3 id="2-4-requests"><a href="#2-4-requests" class="headerlink" title="2.4 requests"></a>2.4 requests</h3><p><a href="http://docs.python-requests.org/zh_CN/latest/user/quickstart.html" target="_blank" rel="noopener">官网地址</a></p>
<p><strong>安装</strong> </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install requests</span><br></pre></td></tr></table></figure>
<p><strong>发送请求</strong>，GET、POST、PUT、PATCH、DELETE </p>
<p>使用请求发送网络请求非常简单。</p>
<p>一开始要导入请求模块：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br></pre></td></tr></table></figure>
<p>然后，尝试获取某个网页。本例子中，我们来获取Github的公共时间线：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">r = requests.get(<span class="string">'https://api.github.com/events'</span>)</span><br></pre></td></tr></table></figure>
<p>请求简便的API意味着所有HTTP请求类型都是显而易见的。例如，你可以这样发送一个HTTP POST请求：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">r = requests.post(<span class="string">'http://httpbin.org/post'</span>, data = &#123;<span class="string">'key'</span>:<span class="string">'value'</span>&#125;)</span><br></pre></td></tr></table></figure>
<p><strong>传递URL参数</strong> </p>
<p>你也许经常想为URL的查询字符串（query string）传递某种数据。如果你是手工构造URL，那么数据会以键/值对的形式置于URL中，跟在一个问号的后面。例如， httpbin.org/get?key=val。</p>
<p>请求允许你使用params关键字参数，以一个字符串字典来提供这些参数。</p>
<p>举例来说，如果你想传递key1 = value1和key2 = value2到httpbin.org/get，那么你可以使用如下代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">payload = &#123;&apos;key1&apos;: &apos;value1&apos;, &apos;key2&apos;: &apos;value2&apos;&#125;</span><br><span class="line"></span><br><span class="line">r = requests.get(&quot;http://httpbin.org/get&quot;, params=payload)</span><br></pre></td></tr></table></figure>
<p>通过打印输出该URL，你能看到URL已被正确确认编码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(r.url)</span><br><span class="line"></span><br><span class="line">http://httpbin.org/get?key2=value2&amp;key1=value1</span><br></pre></td></tr></table></figure>
<p>注意字典里值为无的键都不会被添加到URL的查询字符串里。</p>
<p>你还可以将一个列表作为值传入：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">payload = &#123;&apos;key1&apos;: &apos;value1&apos;, &apos;key2&apos;: [&apos;value2&apos;, &apos;value3&apos;]&#125;</span><br><span class="line"></span><br><span class="line">r = requests.get(&apos;http://httpbin.org/get&apos;, params=payload)</span><br><span class="line"></span><br><span class="line">print(r.url)</span><br><span class="line"></span><br><span class="line">http://httpbin.org/get?key1=value1&amp;key2=value2&amp;key2=value3</span><br></pre></td></tr></table></figure>
<p><strong>响应内容</strong> </p>
<p>我们能读取服务器响应的内容。再次以GitHub时间线为例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">r = requests.get(&apos;https://api.github.com/events&apos;)</span><br><span class="line">r.text</span><br><span class="line"></span><br><span class="line">u&apos;[&#123;&quot;repository&quot;:&#123;&quot;open_issues&quot;:0,&quot;url&quot;:&quot;https://github.com/...</span><br></pre></td></tr></table></figure>
<p>请求会自动解码来自服务器的内容。大多数unicode字符集都能被无缝地解码。</p>
<p>请求发出后，请求会基于HTTP头部对响应的编码作出有根据的推测。当你访问r.text之时，请求会使用其推测的文本编码。你可以找出请求使用了什么编码，并且能够使用r.encoding属性来改变它：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">r.encoding</span><br><span class="line">&apos;utf-8&apos;</span><br><span class="line"></span><br><span class="line">r.encoding = &apos;ISO-8859-1&apos;</span><br></pre></td></tr></table></figure>
<p>如果你改变了编码，每当你访问r.text，请求都将会使用r.encoding的新值。你可能希望在使用特殊逻辑计算出文本的编码的情况下来修改编码。比如HTTP和XML自身可以指定编码。这样的话，你应该使用r.content来找到编码，然后设置r.encoding为相应的编码。这样就能使用正确的编码解析r.text了。</p>
<p>在你需要的情况下，请求也可以使用定制的编码。如果你创建了自己的编码，并使用编解码器模块进行注册，你就可以轻松地使用这个解码器名称作为r.encoding的值，然后由Requests来为你处理编码</p>
<h3 id="2-5-ssl认证"><a href="#2-5-ssl认证" class="headerlink" title="2.5 ssl认证"></a>2.5 ssl认证</h3><p>什么是 SSL 证书？</p>
<p>SSL 证书就是遵守 SSL 安全套接层协议的服务器数字证书。</p>
<p>而 SSL 安全协议最初是由美国网景 Netscape Communication 公司设计开发的，全称为：安全套接层协议 (Secure Sockets Layer) ， 它指定了在应用程序协议 ( 如 HTTP 、 Telnet 、 FTP) 和 TCP/IP 之间提供数据安全性分层的机制，它是在传输通信协议 (TCP/IP) 上实现的一种安全协议，采用公开密钥技术，它为 TCP/IP 连接提供数据加密、服务器认证、消息完整性以及可选的客户机认证。由于此协议很好地解决了互联网明文传输的不安全问题，很快得到了业界的支持，并已经成为国际标准。</p>
<p>SSL 证书由浏览器中“受信任的根证书颁发机构”在验证服务器身份后颁发，具有网站身份验证和加密传输双重功能。</p>
<p>如果能使用 https:// 来访问某个网站，就表示此网站是部署了SSL证书。一般来讲，如果此网站部署了SSL证书，则在需要加密的页面会自动从 http:// 变为 https:// ，如果没有变，你认为此页面应该加密，您也可以尝试直接手动在浏览器地址栏的http后面加上一个英文字母“ s ”后回车，如果能正常访问并出现安全锁，则表明此网站实际上是部署了SSL证书，只是此页面没有做 https:// 链接；如果不能访问，则表明此网站没有部署 SSL证书。</p>
<p>案例:</p>
<p>访问加密的12306的网站</p>
<p>如果不忽略ssl的安全认证的话，网页的源码会提示ssl认证问题，需要提供ssl认证。我们在做爬虫的时候，自动设置忽略掉ssl认证即可。代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import ssl</span><br><span class="line">import urllib.request</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    url = &apos;https://www.12306.cn/mormhweb/&apos;</span><br><span class="line">    # 忽略未经审核的ssl认证</span><br><span class="line">    context = ssl._create_unverified_context()</span><br><span class="line">    res = urllib.request.urlopen(url, context=context)</span><br><span class="line">    print(res.read().decode(&apos;utf-8&apos;))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h2 id="3、数据采集和解析"><a href="#3、数据采集和解析" class="headerlink" title="3、数据采集和解析"></a>3、数据采集和解析</h2><p>通过<a href="./01.网络爬虫和相关工具.md">《网络爬虫和相关工具》</a>一文，我们已经了解到了开发一个爬虫需要做的工作以及一些常见的问题，至此我们可以对爬虫开发需要做的工作以及相关的技术做一个简单的汇总，这其中可能会有一些我们之前没有使用过的第三方库。</p>
<ol>
<li>下载数据 - urllib / requests / aiohttp。</li>
<li>解析数据 - re / lxml / beautifulsoup4（bs4）/ pyquery。</li>
<li>缓存和持久化 - pymysql / sqlalchemy / peewee/ redis / pymongo。</li>
<li>生成数字签名 - hashlib。</li>
<li>序列化和压缩 - pickle / json / zlib。</li>
<li>调度器 - 进程（multiprocessing） / 线程（threading） / 协程（coroutine）。</li>
</ol>
<p><strong>四种采集方式</strong> </p>
<p>四种采集方式的比较</p>
<table>
<thead>
<tr>
<th>抓取方法</th>
<th>速度</th>
<th>使用难度</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>正则表达式</td>
<td>快</td>
<td>困难</td>
<td>常用正则表达式<br>在线正则表达式测试</td>
</tr>
<tr>
<td>lxml</td>
<td>快</td>
<td>一般</td>
<td>需要安装C语言依赖库<br>唯一支持XML的解析器</td>
</tr>
<tr>
<td>Beautiful</td>
<td>较快/较慢（取决于解析器）</td>
<td>简单</td>
<td></td>
</tr>
<tr>
<td>PyQuery</td>
<td>较快</td>
<td>简单</td>
<td>Python版的jQuery</td>
</tr>
</tbody>
</table>
<blockquote>
<p>说明：Beautiful的解析器包括：Python标准库（html.parser）、lxml的HTML解析器、lxml的XML解析器和html5lib。</p>
</blockquote>
<h3 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h3><p>如果你对正则表达式没有任何的概念，那么推荐先阅读<a href="">《正则表达式30分钟入门教程》</a>，然后再阅读我们之前讲解在Python中如何使用正则表达式一文。</p>
<p><strong>re正则匹配</strong> </p>
<p>匹配规则：（原始字符串’booby123’）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">^ 开头	&apos;^b.*&apos;----以b开头的任意字符</span><br><span class="line"></span><br><span class="line">$ 结尾	&apos;^b.*3$&apos;----以b开头，3结尾的任意字符</span><br><span class="line"></span><br><span class="line">* 任意长度（次数），≥0       </span><br><span class="line"></span><br><span class="line">? 非贪婪模式，非贪婪模式尽可能少的匹配所搜索的字符串 &apos;.*?(b.*?b).*&apos;----从左至右第一个b和的二个b之间的内容（包含b）</span><br><span class="line"></span><br><span class="line">+ 一次或多次</span><br><span class="line"></span><br><span class="line">&#123;2&#125; 指定出现次数2次</span><br><span class="line"></span><br><span class="line">&#123;2,&#125; 出现次数≥2次</span><br><span class="line"></span><br><span class="line">&#123;2,5&#125; 出现次数2≤x≤5</span><br><span class="line"></span><br><span class="line">| 或		例如，“z|food”能匹配“z”或“food”(此处请谨慎)。“[z|f]ood”则匹配“zood”或“food”或&quot;zood&quot;。</span><br><span class="line"></span><br><span class="line">[] 中括号中任意一个符合即可（中括号里面没有分转义字符）   &apos;[abc]ooby123&apos;----只要开头符合[]中任意一个即可</span><br><span class="line"></span><br><span class="line">[^] 只要不出现[]的即可</span><br><span class="line"></span><br><span class="line">[a-Z] 从小a到大Z        &apos;1[48357][0-9]&#123;9&#125;&apos;----电话号码</span><br><span class="line"></span><br><span class="line">. 任意字符</span><br><span class="line"></span><br><span class="line">\s 匹配不可见字符 \n \t    &apos;你\s好&apos;----可以匹配‘你 好’</span><br><span class="line"></span><br><span class="line">\S 匹配可见字符，即普通字符</span><br><span class="line"></span><br><span class="line">\w 匹配下划线在内的任何单词字符</span><br><span class="line"></span><br><span class="line">\W 和上一个相反</span><br><span class="line"></span><br><span class="line">[\u4E00-\u9FA5] 只能匹配汉字</span><br><span class="line"></span><br><span class="line">() 要取出的信息就用括号括起来</span><br><span class="line"></span><br><span class="line">\d 数字</span><br></pre></td></tr></table></figure>
<h3 id="XPath语法与Lxml库"><a href="#XPath语法与Lxml库" class="headerlink" title="XPath语法与Lxml库"></a>XPath语法与Lxml库</h3><p><strong>XPATH 术语</strong> </p>
<p><a href="http://www.w3school.com.cn/xpath" target="_blank" rel="noopener">中文文档地址</a></p>
<p>在 XPath 中，有七种类型的节点：元素、属性、文本、命名空间、处理指令、注释以及文档（根）节点。XML 文档是被作为节点树来对待的。树的根被称为文档节点或者根节点。</p>
<p>请看下面这个 XML 文档：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;ISO-8859-1&quot;?&gt;</span><br><span class="line"></span><br><span class="line">&lt;bookstore&gt;</span><br><span class="line"></span><br><span class="line">&lt;book&gt;</span><br><span class="line">  &lt;title lang=&quot;en&quot;&gt;Harry Potter&lt;/title&gt;</span><br><span class="line">  &lt;author&gt;J K. Rowling&lt;/author&gt; </span><br><span class="line">  &lt;year&gt;2005&lt;/year&gt;</span><br><span class="line">  &lt;price&gt;29.99&lt;/price&gt;</span><br><span class="line">&lt;/book&gt;</span><br><span class="line"></span><br><span class="line">&lt;/bookstore&gt;</span><br></pre></td></tr></table></figure>
<p>上面的XML文档中的节点例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;bookstore&gt; （文档节点）</span><br><span class="line">&lt;author&gt;J K. Rowling&lt;/author&gt; （元素节点）</span><br><span class="line">lang=&quot;en&quot; （属性节点） </span><br><span class="line">基本值（或称原子值，Atomic value）</span><br><span class="line">基本值是无父或无子的节点。</span><br></pre></td></tr></table></figure>
<p><strong>节点关系</strong></p>
<p>父（Parent）、子（Children） 每个元素以及属性都有一个父。</p>
<p>例子:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;bookstore&gt;</span><br><span class="line">	&lt;book&gt;</span><br><span class="line">	  &lt;title&gt;Harry Potter&lt;/title&gt;</span><br><span class="line">	  &lt;author&gt;J K. Rowling&lt;/author&gt;</span><br><span class="line">	  &lt;year&gt;2005&lt;/year&gt;</span><br><span class="line">	  &lt;price&gt;29.99&lt;/price&gt;</span><br><span class="line">	&lt;/book&gt;</span><br><span class="line">&lt;/bookstore&gt;</span><br></pre></td></tr></table></figure>
<p>book 元素是 title、author、year 以及 price 元素的父</p>
<p>title、author、year 以及 price 元素都是 book 元素的子</p>
<p>title、author、year 以及 price 元素都是同胞：</p>
<p>title 元素的先辈是 book 元素和 bookstore 元素</p>
<p>bookstore 的后代是 book、title、author、year 以及 price 元素</p>
<p><strong>选取节点</strong></p>
<p>XPath 使用路径表达式在 XML 文档中选取节点。节点是通过沿着路径或者 step 来选取的。</p>
<table>
<thead>
<tr>
<th>表达式</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>nodename</td>
<td>选取此节点的所有子节点。</td>
</tr>
<tr>
<td>/</td>
<td>从根节点选取。</td>
</tr>
<tr>
<td>//</td>
<td>从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置。</td>
</tr>
<tr>
<td>.</td>
<td>选取当前节点。</td>
</tr>
<tr>
<td>..</td>
<td>选取当前节点的父节点。</td>
</tr>
<tr>
<td>@</td>
<td>选取属性。</td>
</tr>
</tbody>
</table>
<p><strong>选取未知节点</strong> </p>
<p>XPath 通配符可用来选取未知的 XML 元素。</p>
<table>
<thead>
<tr>
<th>通配符</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>*</td>
<td>匹配任何元素节点。</td>
</tr>
<tr>
<td>@*</td>
<td>匹配任何属性节点。</td>
</tr>
<tr>
<td>node()</td>
<td>匹配任何类型的节点。</td>
</tr>
</tbody>
</table>
<p><strong>选取若干路径</strong> </p>
<p>通过在路径表达式中使用“|”运算符，您可以选取若干个路径。</p>
<p>爬取搜狐体育：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_code</span><span class="params">(url)</span>:</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36'</span></span><br><span class="line">    &#125;</span><br><span class="line">    req = urllib.request.Request(url, headers=headers)</span><br><span class="line">    res = urllib.request.urlopen(req)</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_crawl</span><span class="params">(url)</span>:</span></span><br><span class="line">    res = get_code(url)</span><br><span class="line">    p1_url = re.findall(<span class="string">"&lt;a test=a href='(.+?)'"</span>, res.read().decode(<span class="string">'GBK'</span>))</span><br><span class="line">    <span class="keyword">return</span> p1_url</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_content</span><span class="params">(url)</span>:</span></span><br><span class="line">    res2 = get_code(url)</span><br><span class="line">    tree = etree.HTML(res2.read().decode(<span class="string">'utf-8'</span>))</span><br><span class="line">    p2_title = tree.xpath(<span class="string">'//*[@id="article-container"]/div[2]/div[1]/div[1]/h1/text()'</span>)</span><br><span class="line">    p2_content = tree.xpath(<span class="string">'//*[@id="mp-editor"]/p/text()'</span>)</span><br><span class="line">    <span class="keyword">return</span> p2_title, p2_content</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line">    url = <span class="string">'http://sports.sohu.com/nba_a.shtml'</span></span><br><span class="line">    p_urls = get_crawl(url)</span><br><span class="line">    <span class="keyword">for</span> p_url <span class="keyword">in</span> p_urls:</span><br><span class="line">        result = get_content(p_url)</span><br><span class="line">        print(result)</span><br></pre></td></tr></table></figure>
<h3 id="BeautifulSoup"><a href="#BeautifulSoup" class="headerlink" title="BeautifulSoup"></a>BeautifulSoup</h3><p>Beautiful Soup 是一个可以从HTML或XML文件中提取数据的Python库.它能够通过你喜欢的转换器实现惯用的文档导航,查找,修改文档的方式.Beautiful Soup会帮你节省数小时甚至数天的工作时间.—–引入<a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html" target="_blank" rel="noopener">官网地址</a>的一句话</p>
<p><strong>安装</strong></p>
<p>Beautiful Soup 4 通过PyPi发布,所以如果你无法使用系统包管理安装,那么也可以通过 easy_install 或 pip 来安装.包的名字是 beautifulsoup4 ,这个包兼容Python2和Python3.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install beautifulsoup4</span><br></pre></td></tr></table></figure>
<p><strong>创建 Beautiful Soup 对象</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup   <span class="comment"># 首先必须要导入 bs4 库</span></span><br><span class="line"></span><br><span class="line">html = <span class="string">"""</span></span><br><span class="line"><span class="string">&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;</span></span><br><span class="line"><span class="string">&lt;body&gt;</span></span><br><span class="line"><span class="string">&lt;p class="story"&gt;...&lt;/p&gt;</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">	</span><br><span class="line">soups  = BeautifulSoup(html)   <span class="comment"># 创建 beautifulsoup 对象</span></span><br><span class="line"><span class="comment"># 另外，我们还可以用本地 HTML 文件来创建对象，例如</span></span><br><span class="line"><span class="comment"># soup = BeautifulSoup(open('index.html'))</span></span><br></pre></td></tr></table></figure>
<p><strong>解析语法、find、find_all</strong></p>
<p><strong>find_all( name , attrs , recursive , text , </strong>kwargs )** </p>
<p>find_all() 方法搜索当前tag的所有tag子节点,并判断是否符合过滤器的条件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1. 查询所有a标签的内容</span><br><span class="line">	soup.find_all(&apos;a&apos;)</span><br><span class="line"></span><br><span class="line">2. 查询所有a标签下class样式为bb的内容</span><br><span class="line">	soup.find_all(&apos;a&apos;, &apos;bb&apos;)</span><br><span class="line"></span><br><span class="line">3. 查询所有id样式为cc的内容</span><br><span class="line">	soup.find_all(id=&apos;cc&apos;</span><br></pre></td></tr></table></figure>
<h3 id="PyQuery"><a href="#PyQuery" class="headerlink" title="PyQuery"></a>PyQuery</h3><p>pyquery相当于jQuery的Python实现，可以用于解析HTML网页。</p>
<p><a href="http://pyquery.readthedocs.io/en/latest/" target="_blank" rel="noopener">官方文档</a><br><a href="http://jquery.cuishifeng.cn/" target="_blank" rel="noopener">jQuery参考文档</a>  </p>
<ol>
<li>存储：mysql、redis、mongodb、sqlalchemy</li>
<li>序列化：json</li>
<li>调度器：进程、线程、协程</li>
</ol>
<p><strong>PyQuery的基本使用</strong>  </p>
<p><strong>1.安装方法 </strong> </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pyquery</span><br></pre></td></tr></table></figure>
<p><strong>2.引用方法</strong>  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from pyquery import PyQuery as pq</span><br></pre></td></tr></table></figure>
<p><strong>3.简介</strong> </p>
<p>　pyquery 是类型jquery 的一个专供python使用的html解析的库，使用方法类似bs4。</p>
<p><strong>4.使用方法</strong> </p>
<p>4.1 初始化方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line">doc =pq(html) <span class="comment">#解析html字符串</span></span><br><span class="line">doc =pq(<span class="string">"http://news.baidu.com/"</span>) <span class="comment">#解析网页</span></span><br><span class="line">doc =pq(<span class="string">"./a.html"</span>) <span class="comment">#解析html 文本</span></span><br></pre></td></tr></table></figure>
<p> 4.2 基本CSS选择器</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from pyquery import PyQuery as pq</span><br><span class="line">html = &apos;&apos;&apos;</span><br><span class="line">    &lt;div id=&quot;wrap&quot;&gt;</span><br><span class="line">        &lt;ul class=&quot;s_from&quot;&gt;</span><br><span class="line">            asdasd</span><br><span class="line">            &lt;link href=&quot;http://asda.com&quot;&gt;asdadasdad12312&lt;/link&gt;</span><br><span class="line">            &lt;link href=&quot;http://asda1.com&quot;&gt;asdadasdad12312&lt;/link&gt;</span><br><span class="line">            &lt;link href=&quot;http://asda2.com&quot;&gt;asdadasdad12312&lt;/link&gt;</span><br><span class="line">        &lt;/ul&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">doc = pq(html)</span><br><span class="line">print doc(&quot;#wrap .s_from link&quot;)</span><br></pre></td></tr></table></figure>
<p>　　运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;link href=&quot;http://asda.com&quot;&gt;asdadasdad12312&lt;/link&gt;</span><br><span class="line">&lt;link href=&quot;http://asda1.com&quot;&gt;asdadasdad12312&lt;/link&gt;</span><br><span class="line">&lt;link href=&quot;http://asda2.com&quot;&gt;asdadasdad12312&lt;/link&gt;</span><br></pre></td></tr></table></figure>
<p>　　#是查找id的标签  .是查找class 的标签  link 是查找link 标签 中间的空格表示里层（注意层级关系以空格隔开） </p>
<p>4.3 查找子元素</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">from pyquery import PyQuery as pq</span><br><span class="line">html = &apos;&apos;&apos;</span><br><span class="line">    &lt;div id=&quot;wrap&quot;&gt;</span><br><span class="line">        &lt;ul class=&quot;s_from&quot;&gt;</span><br><span class="line">            asdasd</span><br><span class="line">            &lt;link href=&quot;http://asda.com&quot;&gt;asdadasdad12312&lt;/link&gt;</span><br><span class="line">            &lt;link href=&quot;http://asda1.com&quot;&gt;asdadasdad12312&lt;/link&gt;</span><br><span class="line">            &lt;link href=&quot;http://asda2.com&quot;&gt;asdadasdad12312&lt;/link&gt;</span><br><span class="line">        &lt;/ul&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">#查找子元素</span><br><span class="line">doc = pq(html)</span><br><span class="line">items=doc(&quot;#wrap&quot;)</span><br><span class="line">print(items)</span><br><span class="line">print(&quot;类型为:%s&quot;%type(items))</span><br><span class="line">link = items.find(&apos;.s_from&apos;)</span><br><span class="line">print(link)</span><br><span class="line">link = items.children()</span><br><span class="line">print(link)</span><br></pre></td></tr></table></figure>
<p>​    运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&lt;div id=&quot;wrap&quot;&gt;</span><br><span class="line">        &lt;ul class=&quot;s_from&quot;&gt;</span><br><span class="line">            asdasd</span><br><span class="line">            &lt;link href=&quot;http://asda.com&quot;&gt;asdadasdad12312&lt;/link&gt;</span><br><span class="line">            &lt;link href=&quot;http://asda1.com&quot;&gt;asdadasdad12312&lt;/link&gt;</span><br><span class="line">            &lt;link href=&quot;http://asda2.com&quot;&gt;asdadasdad12312&lt;/link&gt;</span><br><span class="line">        &lt;/ul&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">类型为:&lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt;</span><br><span class="line">&lt;ul class=&quot;s_from&quot;&gt;</span><br><span class="line">            asdasd</span><br><span class="line">            &lt;link href=&quot;http://asda.com&quot;&gt;asdadasdad12312&lt;/link&gt;</span><br><span class="line">            &lt;link href=&quot;http://asda1.com&quot;&gt;asdadasdad12312&lt;/link&gt;</span><br><span class="line">            &lt;link href=&quot;http://asda2.com&quot;&gt;asdadasdad12312&lt;/link&gt;</span><br><span class="line">        &lt;/ul&gt;</span><br><span class="line">&lt;ul class=&quot;s_from&quot;&gt;</span><br><span class="line">            asdasd</span><br><span class="line">            &lt;link href=&quot;http://asda.com&quot;&gt;asdadasdad12312&lt;/link&gt;</span><br><span class="line">            &lt;link href=&quot;http://asda1.com&quot;&gt;asdadasdad12312&lt;/link&gt;</span><br><span class="line">            &lt;link href=&quot;http://asda2.com&quot;&gt;asdadasdad12312&lt;/link&gt;</span><br><span class="line">        &lt;/ul&gt;</span><br></pre></td></tr></table></figure>
<p>根据运行结果可以发现返回结果类型为pyquery，并且find方法和children 方法都可以获取里层标签</p>
<p> 4.4查找父元素 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">doc = pq(html)</span><br><span class="line">items=doc(<span class="string">".s_from"</span>)</span><br><span class="line">print(items)</span><br><span class="line"><span class="comment">#查找父元素</span></span><br><span class="line">parent_href=items.parent()</span><br></pre></td></tr></table></figure>
<p>parent可以查找出外层标签包括的内容，与之类似的还有parents,可以获取所有外层节点 </p>
<p>4.5 查找兄弟元素 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">doc = pq(html)</span><br><span class="line">items=doc(&quot;link.active1.a123&quot;)</span><br><span class="line">print(items)</span><br><span class="line">#查找兄弟元素</span><br><span class="line">siblings_href=items.siblings()</span><br></pre></td></tr></table></figure>
<p>​    根据运行结果可以看出，siblings 返回了同级的其他标签<br>    结论：子元素查找，父元素查找，兄弟元素查找，这些方法返回的结果类型都是pyquery类型，可以针对结果再次进行选择</p>
<p>4.6 遍历查找结果 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">doc = pq(html)</span><br><span class="line">its=doc(&quot;link&quot;).items()</span><br><span class="line">for it in its:</span><br><span class="line">    print(it)</span><br></pre></td></tr></table></figure>
<p>​    运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;link class=&quot;active1 a123&quot; href=&quot;http://asda.com&quot;&gt;asdadasdad12312&lt;/link&gt;</span><br><span class="line">&lt;link class=&quot;active2&quot; href=&quot;http://asda1.com&quot;&gt;asdadasdad12312&lt;/link&gt;          </span><br><span class="line">&lt;link class=&quot;movie1&quot; href=&quot;http://asda2.com&quot;&gt;asdadasdad12312&lt;/link&gt;</span><br></pre></td></tr></table></figure>
<p>4.7获取属性信息 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">doc = pq(html)</span><br><span class="line">its=doc(&quot;link&quot;).items()</span><br><span class="line">for it in its:</span><br><span class="line">    print(it.attr(&apos;href&apos;))</span><br><span class="line">    print(it.attr.href)   #两种写法结果都一样</span><br></pre></td></tr></table></figure>
<p>4.8 获取文本 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">doc = pq(html)</span><br><span class="line">its=doc(&quot;link&quot;).items()</span><br><span class="line">for it in its:</span><br><span class="line">    print(it.text())</span><br></pre></td></tr></table></figure>
<p>4.9 获取 HTML信息 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">doc = pq(html)</span><br><span class="line">its=doc(&quot;link&quot;).items()</span><br><span class="line">for it in its:</span><br><span class="line">    print(it.html())</span><br></pre></td></tr></table></figure>
<p>​    运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;a&gt;asdadasdad12312&lt;/a&gt;</span><br><span class="line">asdadasdad12312</span><br><span class="line">asdadasdad12312</span><br></pre></td></tr></table></figure>
<p><strong>5.常用DOM操作</strong>  </p>
<p>5.1 addClass removeClass</p>
<p>　　添加，移除class标签</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">doc = pq(html)</span><br><span class="line">its=doc(&quot;link&quot;).items()</span><br><span class="line">for it in its:</span><br><span class="line">    print(&quot;添加:%s&quot;%it.addClass(&apos;active1&apos;))</span><br><span class="line">    print(&quot;移除:%s&quot;%it.removeClass(&apos;active1&apos;))</span><br></pre></td></tr></table></figure>
<p>需要注意的是已经存在的class标签不会继续添加 </p>
<p>5.2 attr css</p>
<p>　　attr 为获取/修改属性 css 添加style属性</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">doc = pq(html)</span><br><span class="line">its=doc(&quot;link&quot;).items()</span><br><span class="line">for it in its:</span><br><span class="line">    print(&quot;修改:%s&quot;%it.attr(&apos;class&apos;,&apos;active&apos;))</span><br><span class="line">    print(&quot;添加:%s&quot;%it.css(&apos;font-size&apos;,&apos;14px&apos;)</span><br></pre></td></tr></table></figure>
<p>attr css操作直接修改对象的 </p>
<p>5.3 remove 移除标签 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">doc = pq(html)</span><br><span class="line">its=doc(&quot;div&quot;)</span><br><span class="line">print(&apos;移除前获取文本结果:\n%s&apos;%its.text())</span><br><span class="line">it=its.remove(&apos;ul&apos;)</span><br><span class="line">print(&apos;移除后获取文本结果:\n%s&apos;%it.text())</span><br></pre></td></tr></table></figure>
<h2 id="4、并发、并行、同步、异步线程、进程"><a href="#4、并发、并行、同步、异步线程、进程" class="headerlink" title="4、并发、并行、同步、异步线程、进程"></a>4、并发、并行、同步、异步线程、进程</h2><h3 id="4-1-同步和异步、阻塞和非阻塞"><a href="#4-1-同步和异步、阻塞和非阻塞" class="headerlink" title="4.1 同步和异步、阻塞和非阻塞"></a>4.1 同步和异步、阻塞和非阻塞</h3><h4 id="同步和异步"><a href="#同步和异步" class="headerlink" title="同步和异步"></a>同步和异步</h4><p>同步和异步是相对于操作结果来说，会不会等待结果</p>
<h4 id="阻塞和非阻塞"><a href="#阻塞和非阻塞" class="headerlink" title="阻塞和非阻塞"></a>阻塞和非阻塞</h4><p>阻塞是在煮稀饭的过程中，你不能去干其他的事情。非阻塞是在煮稀饭的过程中，你还可以去做其他的事情。阻塞和非阻塞是相对于线程是否被阻塞</p>
<h4 id="同步和阻塞的区别"><a href="#同步和阻塞的区别" class="headerlink" title="同步和阻塞的区别"></a>同步和阻塞的区别</h4><p>同步是一个过程，阻塞是线程的一个状态。</p>
<p>当多个线程操作同一公共变量的时候可能会出现竞争的情况，这时候需要使用同步来防止多个线程同时占用资源的情况，让一个线程在运行状态中，另外的线程处于就绪状态，当前一个线程处于暂停状态的时候，后面的处于就绪状态的线程，获取到资源以后，获取到时间片以后就会处于运行状态了。所以阻塞是线程的一个状态而已</p>
<h4 id="并发和并行"><a href="#并发和并行" class="headerlink" title="并发和并行"></a>并发和并行</h4><p>并发：从点餐系统看，该肯德基店只有一个负责点餐的收银员，而有2台收银点餐设备，服务员同时操作2个收银点餐终端，这叫并发操作收银点餐终端。</p>
<p>并行：肯德基为了拓展业务，提高同时服务的能力，在全世界开设分店，这叫并行。</p>
<p>如何实现并发呢：需要引入多进程，多线程，协程</p>
<h3 id="4-2-进程"><a href="#4-2-进程" class="headerlink" title="4.2 进程"></a>4.2 进程</h3><p><strong>概念</strong>：</p>
<p>进程即正在执行的一个过程。进程是对正在运行程序的一个抽象。操作系统以进程为单位分配存储空间，每个进程都有自己的地址空间、数据栈以及其他用于跟踪进程执行的辅助数据，操作系统管理所有进程的执行，为它们合理的分配资源。进程可以通过fork或spawn的方式来创建新的进程来执行其他的任务，不过新的进程也有自己独立的内存空间，因此必须通过进程间通信机制（IPC，Inter-Process Communication）来实现数据共享，具体的方式包括管道、信号、套接字、共享内存区等。</p>
<p><strong>python实现进程</strong>：</p>
<p>multiprocessing模块就是跨平台版本的多进程模块。</p>
<p>multiprocessing模块提供了一个Process类来代表一个进程对象，</p>
<p>代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import time</span><br><span class="line">from random import randint</span><br><span class="line">from multiprocessing import Process</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def coding():</span><br><span class="line">    while True:</span><br><span class="line">        print(&apos;开始撸代码，PID是%s&apos; % os.getpid())</span><br><span class="line">        time.sleep(randint(1, 3))</span><br><span class="line">        print(&apos;写累了，不撸了，PID是%s&apos; % os.getpid())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def play_weixin():</span><br><span class="line">    while True:</span><br><span class="line">        print(&apos;玩一会微信，PID是%s&apos; % os.getpid())</span><br><span class="line">        time.sleep(randint(1,2))</span><br><span class="line">        print(&apos;不玩微信了，开始撸代码，PID是%s&apos; % os.getpid())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line"></span><br><span class="line">    # 创建进程</span><br><span class="line">    p1 = Process(target=coding)</span><br><span class="line">    p2 = Process(target=coding)</span><br><span class="line">    p3 = Process(target=play_weixin)</span><br><span class="line"></span><br><span class="line">    # 启动进程</span><br><span class="line">    p1.start()</span><br><span class="line">    # 阻塞进程p1</span><br><span class="line">    p1.join()</span><br><span class="line">	</span><br><span class="line">	# 启动进程</span><br><span class="line">    p2.start()</span><br><span class="line">    p3.start()</span><br><span class="line">	</span><br><span class="line">	# 主进程</span><br><span class="line">    while True:</span><br><span class="line">        time.sleep(3)</span><br><span class="line">        print(&apos;我是主进程，PID是%s&apos; % os.getpid())</span><br></pre></td></tr></table></figure>
<h4 id="杀掉进程"><a href="#杀掉进程" class="headerlink" title="杀掉进程"></a>杀掉进程</h4><p>按照上面案例代码运行的话，p1进程会一直阻塞，后面的p2和p3并不会执行。如果在windows中运行的代码，则直接运行‘启动任务管理器’去杀掉进程，这时候p2和p3的进程就会执行了，说明进程之间是相互没有关联的，互不影响的。如果在linux系统中，直接kill -9 PID，就可以杀掉进程了</p>
<h3 id="4-3-线程"><a href="#4-3-线程" class="headerlink" title="4.3 线程"></a>4.3 线程</h3><p>一个进程中的多个线程可以共享一个资源内存空间</p>
<p>Python的标准库提供了两个模块：thread和threading，thread是低级模块，threading是高级模块，对thread进行了封装。绝大多数情况下，我们只需要使用threading这个高级模块。</p>
<p>启动一个线程,创建threading的实例，然后直接start()就可以启动我们定义的线程了。</p>
<h4 id="多线程"><a href="#多线程" class="headerlink" title="多线程"></a>多线程</h4><p>定义一个线程类，继承自threading.Thread</p>
<p>其中打印一下当前线程的名称，使用threading.current_thread().name来获取当前线程的名称。默认的Python就自动给线程命名为Thread-1，Thread-2……。当然我们也可以自定义线程的名称</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import threading</span><br><span class="line"></span><br><span class="line">class DataCopy(threading.Thread):</span><br><span class="line"></span><br><span class="line">    def __init__(self, dbname):</span><br><span class="line">        super(DataCopy, self).__init__()</span><br><span class="line">        self.dbName = dbname</span><br><span class="line"></span><br><span class="line">    def run(self):</span><br><span class="line"></span><br><span class="line">		print(&apos;Thread %s is running&apos; % threading.current_thread().name)</span><br><span class="line">        print(&apos;开始备份数据库:%s&apos; % self.dbName)</span><br><span class="line"></span><br><span class="line">        time.sleep(5)</span><br><span class="line"></span><br><span class="line">        print(&apos;数据库%s备份结束&apos; % self.dbName)</span><br><span class="line">		print(&apos;Thread %s is ended&apos; % threading.current_thread().name)</span><br></pre></td></tr></table></figure>
<p>启动一个线程：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">thread1 = DataCopy(&apos;database1&apos;)</span><br><span class="line"></span><br><span class="line">thread1.start()</span><br><span class="line"></span><br><span class="line"># 线程执行结束的输出提示</span><br><span class="line">print(&apos;备份结束&apos;)</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<p><img src="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/threading_many_join.png" alt=""></p>
<h4 id="守护线程"><a href="#守护线程" class="headerlink" title="守护线程"></a>守护线程</h4><p>当定义子线程为守护线程的话，当主线程结束了，不管子线程是否执行完，都会被直接给暂停掉。默认daemon为False</p>
<p>代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">thread1 = DataCopy(&apos;database1&apos;)</span><br><span class="line">thread2 = DataCopy(&apos;database2&apos;)</span><br><span class="line"></span><br><span class="line"># 设置守护线程</span><br><span class="line">thread1.daemon = True</span><br><span class="line">thread2.daemon = True</span><br><span class="line"></span><br><span class="line"># 运行线程</span><br><span class="line">thread1.start()</span><br><span class="line">thread2.start()</span><br><span class="line"></span><br><span class="line"># 线程执行结束的输出提示</span><br><span class="line">print(&apos;备份结束&apos;)</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/threading_many_deamon.png" alt=""></p>
<h4 id="线程启动"><a href="#线程启动" class="headerlink" title="线程启动"></a>线程启动</h4><p>解释: start和run的区别</p>
<p>start() 方法是启动一个子线程，线程名就是我们定义的name，或者默认的线程名Thread-1， Thread-2……</p>
<p>run() 方法并不启动一个新线程，就是在主线程中调用了一个普通函数而已。</p>
<p>代码1，先使用start()启动线程，并且打印当前线程的名称：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">thread1 = DataCopy(&apos;database1&apos;)</span><br><span class="line">thread2 = DataCopy(&apos;database2&apos;)</span><br><span class="line"></span><br><span class="line"># 使用start启动，两个线程同时执行</span><br><span class="line">thread1.start()</span><br><span class="line">thread2.start()</span><br><span class="line"></span><br><span class="line"># 线程执行结束的输出提示</span><br><span class="line">print(&apos;备份结束&apos;)</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<p><img src="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/threading_many_start.png" alt=""></p>
<p>代码2，使用run()启动线程，并且打印当前线程的名称：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">thread1 = DataCopy(&apos;database1&apos;)</span><br><span class="line">thread2 = DataCopy(&apos;database2&apos;)</span><br><span class="line">	</span><br><span class="line">thread1.run()</span><br><span class="line">thread2.run()</span><br><span class="line"></span><br><span class="line"># 线程执行结束的输出提示</span><br><span class="line">print(&apos;备份结束&apos;)</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<p><img src="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/threading_many_run.png" alt=""></p>
<h3 id="4-4-线程锁"><a href="#4-4-线程锁" class="headerlink" title="4.4 线程锁"></a>4.4 线程锁</h3><p>使用线程时最不愿意遇到的情况就是多个线程竞争资源，在这种情况下为了保证资源状态的正确性，我们可能需要对资源进行加锁保护的处理，这一方面会导致程序失去并发性，另外如果多个线程竞争多个资源时，还有可能因为加锁方式的不当导致死锁。</p>
<p>要实现将资源和持有资源的线程进行绑定的操作，最简单的做法就是使用threading模块的local类，在网络爬虫开发中，就可以使用local类为每个线程绑定一个MySQL数据库连接或Redis客户端对象，这样通过线程可以直接获得这些资源，既解决了资源竞争的问题，又避免了在函数和方法调用时传递这些资源。</p>
<h4 id="锁的概念"><a href="#锁的概念" class="headerlink" title="锁的概念"></a>锁的概念</h4><p>线程锁：其实并不是给资源加锁, 而是用锁去锁定资源，你可以定义多个锁,当你需要独占某一资源时，任何一个锁都可以锁这个资源，就好比你用不同的锁都可以把相同的一个门锁住是一个道理</p>
<p>基本语法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#创建锁</span><br><span class="line">mutex = threading.Lock()</span><br><span class="line">#锁定</span><br><span class="line">mutex.acquire([timeout])</span><br><span class="line">#释放</span><br><span class="line">mutex.release()</span><br></pre></td></tr></table></figure>
<p>代码：怎么使用锁去锁住资源，释放锁</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">import threading</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">counter = 0</span><br><span class="line"># 只是定义一个锁,并不是给资源加锁,你可以定义多个锁,像下两行代码,当你需要占用这个资源时，任何一个锁都可以锁这个资源</span><br><span class="line">counter_lock = threading.Lock()</span><br><span class="line">counter_lock2 = threading.Lock()</span><br><span class="line">counter_lock3 = threading.Lock()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 可以使用上边三个锁的任何一个来锁定资源</span><br><span class="line"></span><br><span class="line">class MyThread(threading.Thread):</span><br><span class="line">    # 使用类定义thread，继承threading.Thread</span><br><span class="line">    def __init__(self, name):</span><br><span class="line">        super(MyThread, self).__init__()</span><br><span class="line">        self.name = &quot;Thread-&quot; + str(name)</span><br><span class="line"></span><br><span class="line">    def run(self):  # run函数必须实现</span><br><span class="line">        # 多线程是共享资源的，使用全局变量</span><br><span class="line">        global counter, counter_lock</span><br><span class="line">        # 当需要独占counter资源时，必须先锁定，这个锁可以是任意的一个锁，可以使用上边定义的3个锁中的任意一个</span><br><span class="line">        time.sleep(1);</span><br><span class="line">        if counter_lock.acquire():</span><br><span class="line">            counter += 1</span><br><span class="line">            print(&quot;I am %s, set counter:%s&quot; % (self.name, counter))</span><br><span class="line">            # 使用完counter资源必须要将这个锁打开，让其他线程使用</span><br><span class="line">            counter_lock.release()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    for i in range(1, 101):</span><br><span class="line">        my_thread = MyThread(i)</span><br><span class="line">        my_thread.start()</span><br></pre></td></tr></table></figure>
<p>以上代码，只是教会你怎么去使用锁，锁住资源，当一个线程在使用锁住的资源的时候，其他线程则无法再使用该资源了，起到了很好的避免资源的竞争。</p>
<p><strong>多线程去打印输出自增的全局变量</strong> </p>
<p>定义一个简单的多线程，主要功能是用于打印全局递增的变量参数，并打印线程名。</p>
<p>可以不妨想想，如果启动多线程对同一全局变量进行递增操作的话，就有可能多个线程同时获取到全局变量，然后进行递增操作。可想而知，如果是这样的话，那么打印出来的变量就有可能会重复。这就是出现了一个进程中，多线程同时共享一个资源的时候，出现的资源竞争的问题了。我们先查看一下代码的运行结果，然后进行分析：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">import threading</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class MyThread(threading.Thread):</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        threading.Thread.__init__(self)</span><br><span class="line"></span><br><span class="line">    def run(self):</span><br><span class="line">        global n, lock</span><br><span class="line"></span><br><span class="line">		# 休眠1秒钟</span><br><span class="line">        time.sleep(1)</span><br><span class="line">		</span><br><span class="line">		# 线程主要打印循环地址的n值，和对于的线程的名称，线程名也可以使用self.name来获取</span><br><span class="line">        print(n, threading.current_thread().name)</span><br><span class="line">        n += 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">	</span><br><span class="line">	# 设置全局变量</span><br><span class="line">    n = 1</span><br><span class="line">    ThreadList = []</span><br><span class="line">	# 创建线程锁</span><br><span class="line">    lock = threading.Lock()</span><br><span class="line"></span><br><span class="line">	# 创建20个线程</span><br><span class="line">    for i in range(1, 20):</span><br><span class="line">        t = MyThread()</span><br><span class="line">        ThreadList.append(t)</span><br><span class="line">	</span><br><span class="line">	# 启动线程</span><br><span class="line">    for t in ThreadList:</span><br><span class="line">        t.start()</span><br><span class="line">	</span><br><span class="line">	# 阻塞线程</span><br><span class="line">    for t in ThreadList:</span><br><span class="line">        t.join()</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">1 Thread-2</span><br><span class="line">1 Thread-3</span><br><span class="line">1 Thread-1</span><br><span class="line">4 Thread-7</span><br><span class="line">4 Thread-5</span><br><span class="line">4 Thread-9</span><br><span class="line">4 Thread-4</span><br><span class="line">5 Thread-6</span><br><span class="line">6 Thread-11</span><br><span class="line">7 Thread-10</span><br><span class="line">9 Thread-8</span><br><span class="line">12 Thread-15</span><br><span class="line">12 Thread-19</span><br><span class="line">12 Thread-18</span><br><span class="line">12 Thread-16</span><br><span class="line">12 Thread-17</span><br><span class="line">14 Thread-12</span><br><span class="line">18 Thread-14</span><br><span class="line">19 Thread-13</span><br><span class="line"></span><br><span class="line">Process finished with exit code 0</span><br></pre></td></tr></table></figure>
<p>分析：</p>
<p>很明显的在我们的执行结果中，打印的全局变量n中有很多重复的，这就印证了我们的多线程在共享资源上存在不可避免的资源竞争的关系。为了解决这种资源的竞争，我们可以采取线程锁的形式去避免对资源的竞争</p>
<p><strong>优化多线程打印输出自增的全局变量</strong> </p>
<p>代码优化：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def run(self):</span><br><span class="line">    global n, lock</span><br><span class="line">    time.sleep(1)</span><br><span class="line"></span><br><span class="line">	# 判断是否锁定了资源</span><br><span class="line">    if lock.acquire():</span><br><span class="line">		</span><br><span class="line">		# 如果锁定了资源，则打印如下的全局变量n和线程名，并且全局变量n自增1</span><br><span class="line">        print(n, self.name)</span><br><span class="line">        n += 1</span><br><span class="line"></span><br><span class="line">		# 释放锁</span><br><span class="line">        lock.release()</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">1 Thread-2</span><br><span class="line">2 Thread-3</span><br><span class="line">3 Thread-1</span><br><span class="line">4 Thread-4</span><br><span class="line">5 Thread-5</span><br><span class="line">6 Thread-8</span><br><span class="line">7 Thread-9</span><br><span class="line">8 Thread-12</span><br><span class="line">9 Thread-13</span><br><span class="line">10 Thread-6</span><br><span class="line">11 Thread-16</span><br><span class="line">12 Thread-17</span><br><span class="line">13 Thread-18</span><br><span class="line">14 Thread-7</span><br><span class="line">15 Thread-19</span><br><span class="line">16 Thread-11</span><br><span class="line">17 Thread-10</span><br><span class="line">18 Thread-15</span><br><span class="line">19 Thread-14</span><br><span class="line"></span><br><span class="line">Process finished with exit code 0</span><br></pre></td></tr></table></figure>
<p>分析：在优化的代码中，我们先建立了一个threading.Lock类对象lock,在run方法里，我们使用lock.acquire()获得了这个锁。此时，其他的线程就无法再获得该锁了，他们就会阻塞在“if lock.acquire()”这里，直到锁被另一个线程释放：lock.release()。所以，if语句中的内容就是一块完整的代码，不会再存在执行了一半就暂停去执行别的线程的情况。所以最后结果是整齐的。完美的解决了多线程竞争同一资源造成的问题了。</p>
<h2 id="5、异步I-O"><a href="#5、异步I-O" class="headerlink" title="5、异步I/O"></a>5、异步I/O</h2><h3 id="协程的概念"><a href="#协程的概念" class="headerlink" title="协程的概念"></a>协程的概念</h3><p>协程（coroutine）通常又称之为微线程或纤程，它是相互协作的一组子程序（函数）。所谓相互协作指的是在执行函数A时，可以随时中断去执行函数B，然后又中断继续执行函数A。注意，这一过程并不是函数调用（因为没有调用语句），整个过程看似像多线程，然而协程只有一个线程执行。协程通过<code>yield</code>关键字和 <code>send()</code>操作来转移执行权，协程之间不是调用者与被调用者的关系。</p>
<p>协程的优势在于以下两点：</p>
<ol>
<li>执行效率极高，因为子程序（函数）切换不是线程切换，由程序自身控制，没有切换线程的开销。</li>
<li>不需要多线程的锁机制，因为只有一个线程，也不存在竞争资源的问题，当然也就不需要对资源加锁保护，因此执行效率高很多。</li>
</ol>
<blockquote>
<p>说明：协程适合处理的是I/O密集型任务，处理CPU密集型任务并不是它的长处，如果要提升CPU的利用率可以考虑“多进程+协程”的模式。</p>
</blockquote>
<p><strong>历史回顾</strong> </p>
<ol>
<li>Python 2.2：第一次提出了生成器（最初称之为迭代器）的概念（PEP 255）。</li>
<li>Python 2.5：引入了将对象发送回暂停了的生成器这一特性即生成器的<code>send()</code>方法（PEP 342）。</li>
<li>Python 3.3：添加了<code>yield from</code>特性，允许从迭代器中返回任何值（注意生成器本身也是迭代器），这样我们就可以串联生成器并且重构出更好的生成器。</li>
<li>Python 3.4：引入<code>asyncio.coroutine</code>装饰器用来标记作为协程的函数，协程函数和<code>asyncio</code>及其事件循环一起使用，来实现异步I/O操作。</li>
<li>Python 3.5：引入了<code>async</code>和<code>await</code>，可以使用<code>async def</code>来定义一个协程函数，这个函数中不能包含任何形式的<code>yield</code>语句，但是可以使用<code>return</code>或<code>await</code>从协程中返回值。</li>
</ol>
<p><strong>在将协程的时候，需要分别引入迭代器和生成器的含义以及案例</strong></p>
<p><strong>迭代器</strong> </p>
<p>迭代器和生成器都是python中最重要的知识点。迭代器可以遍历整个对象，在你需要取值的时候，调用next就可以依次获取迭代器中的下一个值，在迭代中中只能往下取值，不能再往上取值的</p>
<p>案例代码：</p>
<p>声明一个列表[1，2，3，4]，然后使用iter()去创建一个迭代器，然后依次调用<strong>next</strong>()就可以获取到迭代器中的下一个值了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">s = iter([1, 2, 3, 4])</span><br><span class="line">print(type([1, 2, 3, 4]))</span><br><span class="line"></span><br><span class="line">print(type(s))</span><br><span class="line">print(s.__next__())</span><br><span class="line">print(s.__next__())</span><br><span class="line">print(s.__next__())</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;class &apos;list&apos;&gt;</span><br><span class="line">&lt;class &apos;list_iterator&apos;&gt;</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line"></span><br><span class="line">Process finished with exit code 0</span><br></pre></td></tr></table></figure>
<p><strong>生成器</strong> </p>
<p>什么是生成器：使用了 yield 的函数被称为生成器（generator）。生成器函数返回的结果就是一个迭代器，只能用于迭代操作。既然是迭代器了，就有<strong>next</strong>()的属性了。</p>
<p>那生成器是怎么工作的呢：在调用生成器运行的过程中，当在第一次运行的时候，在遇到yield时函数会暂停并保持当前所有的运行信息，返回一个yield的值，当再次<strong>next</strong>()的时候，才会在当前代码位置进行运行。</p>
<p>案例代码： 实现斐波那契算法 0 1 1 2 3 5 8 13 21 34 55</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def fibonacci(n):</span><br><span class="line">    # 生成器函数 - 斐波那契</span><br><span class="line">    a, b, counter = 0, 1, 0</span><br><span class="line">    while True:</span><br><span class="line">        if counter &gt; n:</span><br><span class="line">            return</span><br><span class="line">        yield a</span><br><span class="line">        a, b = b, a + b</span><br><span class="line">        counter += 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line"></span><br><span class="line">    f = fibonacci(10)  # f 是一个迭代器，由生成器返回生成</span><br><span class="line"></span><br><span class="line">    while True:</span><br><span class="line">        try:</span><br><span class="line">            # print(next(f), end=&quot; &quot;)</span><br><span class="line">            print(f.__next__(), end=&quot; &quot;)</span><br><span class="line">        except StopIteration:</span><br><span class="line">            # 如果获取到最后一个的时候，再获取next就会提示StopIteration的异常了</span><br><span class="line">            sys.exit()</span><br></pre></td></tr></table></figure>
<p>运行结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">0 1 1 2 3 5 8 13 21 34 55 </span><br><span class="line"></span><br><span class="line">Process finished with exit code 0</span><br></pre></td></tr></table></figure>
<p>执行线路图：</p>
<p>查看执行的流程可以使用debug模式去运行，通过断点调试可以很清晰的了解到整个代码的运行流程。</p>
<p><img src="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/yield_shenchengqi.png" alt=""></p>
<p><strong>协程例子</strong> </p>
<p>案例代码： 消费者和生产者模式</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">import time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def consumer():</span><br><span class="line">    r = &apos;1xx&apos;</span><br><span class="line">    while True:</span><br><span class="line">        n = yield r</span><br><span class="line">        if not n:</span><br><span class="line">            return</span><br><span class="line">        print(&apos;[CONSUMER] 吃鸡翅 %s...&apos; % n)</span><br><span class="line">        time.sleep(1)</span><br><span class="line">        r = &apos;吃完啦，饱饱的了&apos;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def produce(customer):</span><br><span class="line">    # 启动迭代器</span><br><span class="line">    customer.__next__()</span><br><span class="line">    # 设置变量参数为0</span><br><span class="line">    n = 0</span><br><span class="line">    while n &lt; 3:</span><br><span class="line">        n = n + 1</span><br><span class="line">        print(&apos;[PRODUCER] 做鸡翅 %s...&apos; % n)</span><br><span class="line">        # 想customer中传递变量n，直接跳到consumer中执行</span><br><span class="line">        r = customer.send(n)</span><br><span class="line">        print(&apos;[PRODUCER] 吃鸡翅状态 return: %s&apos; % r)</span><br><span class="line">    # 关闭消费者</span><br><span class="line">    customer.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    print(&apos;开始协程&apos;)</span><br><span class="line">    customer = consumer()</span><br><span class="line">    produce(customer)</span><br><span class="line">    print(&apos;结束协程&apos;)</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">开始协程</span><br><span class="line">[PRODUCER] 做鸡翅 1...</span><br><span class="line">[CONSUMER] 吃鸡翅 1...</span><br><span class="line">[PRODUCER] 吃鸡翅状态 return: 吃完啦，饱饱的了</span><br><span class="line">[PRODUCER] 做鸡翅 2...</span><br><span class="line">[CONSUMER] 吃鸡翅 2...</span><br><span class="line">[PRODUCER] 吃鸡翅状态 return: 吃完啦，饱饱的了</span><br><span class="line">[PRODUCER] 做鸡翅 3...</span><br><span class="line">[CONSUMER] 吃鸡翅 3...</span><br><span class="line">[PRODUCER] 吃鸡翅状态 return: 吃完啦，饱饱的了</span><br><span class="line">结束协程</span><br><span class="line"></span><br><span class="line">Process finished with exit code 0</span><br></pre></td></tr></table></figure>
<p>代码分析：</p>
<ol>
<li><p>在获取迭代器的时候，有三种获取方式</p>
<p>next(customer)</p>
<p>customer.send(None)</p>
<p>customer.<strong>next</strong>()</p>
</li>
</ol>
<p>注意:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">customer函数是一个generator（生成器），把一个customer传入produce后：</span><br><span class="line"></span><br><span class="line">首先调用next()启动生成器；</span><br><span class="line"></span><br><span class="line">然后，一旦生产了东西，通过customer.send(n)切换到consumer执行；</span><br><span class="line"></span><br><span class="line">customer通过yield拿到消息，处理，又通过yield把结果传回；</span><br><span class="line"></span><br><span class="line">produce拿到customer处理的结果，继续生产下一条消息；</span><br><span class="line"></span><br><span class="line">produce决定不生产了，通过c.close()关闭consumer，整个过程结束。</span><br><span class="line"></span><br><span class="line">整个流程无锁，由一个线程执行，produce和consumer协作完成任务，所以称为“协程”，而非线程的抢占式多任务。</span><br></pre></td></tr></table></figure>
<p>重点：区分next()和send()的区别</p>
<p>其实next()和send()在一定意义上作用是相似的，区别是send()可以传递yield表达式的值进去，而next()不能传递特定的值，只能传递None进去。因此，我们可以看做next() 和 send(None) 作用是一样的。需要提醒的是，第一次调用时，请使用next()语句或是send(None)，不能使用send发送一个非None的值，否则会直接报错</p>
<h3 id="aiohttp"><a href="#aiohttp" class="headerlink" title="aiohttp"></a>aiohttp</h3><p><a href="http://aiohttp.readthedocs.io/en/stable/" target="_blank" rel="noopener">官方文档</a></p>
<p>aiohttp是什么，官网上有这样一句话介绍：Async HTTP client/server for asyncio and Python，是异步的HTTP框架 </p>
<p><strong>安装</strong> </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install aiohttp</span><br></pre></td></tr></table></figure>
<p><strong>爬取豆瓣电影资源</strong> </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">import aiohttp</span><br><span class="line">import json</span><br><span class="line">import asyncio</span><br><span class="line">from pymongo import MongoClient</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class DouBan(object):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.tag_url = &apos;https://movie.douban.com/j/search_tags?type=movie&amp;source=&apos;</span><br><span class="line">        self.bash_url = &apos;https://movie.douban.com/j/search_subjects?type=movie&amp;tag=&#123;tag&#125;&amp;sort=recommend&amp;page_limit=20&amp;page_start=&#123;page_start&#125;&apos;</span><br><span class="line">        self.tag_key = []</span><br><span class="line">        self.max_page = 10</span><br><span class="line">        client = MongoClient(host=&apos;127.0.0.1&apos;, port=27017)</span><br><span class="line">        db = client[&apos;unsplash&apos;]</span><br><span class="line">        self.collection = db[&apos;images&apos;]</span><br><span class="line"></span><br><span class="line">    async def get_img_info(self):</span><br><span class="line">        async with aiohttp.ClientSession() as session:</span><br><span class="line">            # 获取电影分类的信息</span><br><span class="line">            async with session.get(self.tag_url) as tag_rsponse:</span><br><span class="line">                self.tag_key = self.parse_tag(await tag_rsponse.text())</span><br><span class="line">            print(self.tag_key)</span><br><span class="line">            # 循环去获取网页api内容信息</span><br><span class="line">            for key in self.tag_key:</span><br><span class="line">                for page in range(0, self.max_page):</span><br><span class="line">                    async with session.get(self.bash_url.format(tag=key, page_start=page*20)) as response:</span><br><span class="line">                        await self.parse(await response.text())</span><br><span class="line"></span><br><span class="line">    def parse_tag(self, response):</span><br><span class="line">        json_data = json.loads(response)[&apos;tags&apos;]</span><br><span class="line">        return json_data</span><br><span class="line"></span><br><span class="line">    async def parse(self,response):</span><br><span class="line">        json_data = json.loads(response)[&apos;subjects&apos;]</span><br><span class="line">        for data in json_data:</span><br><span class="line">            await self.do_insert(data)</span><br><span class="line"></span><br><span class="line">    async def do_insert(self, document):</span><br><span class="line">        try:</span><br><span class="line">            result = self.collection.insert_one(document)</span><br><span class="line">        except BaseException as e:</span><br><span class="line">            print(&apos;error%s&apos; % e)</span><br><span class="line">        else:</span><br><span class="line">            print(&apos;result %s&apos; % repr(result.inserted_id))</span><br><span class="line"></span><br><span class="line">    def run(self):</span><br><span class="line">        loop = asyncio.get_event_loop()</span><br><span class="line">        tasks = [self.get_img_info()]</span><br><span class="line">        loop.run_until_complete(asyncio.wait(tasks))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    us = DouBan()</span><br><span class="line">    us.run()</span><br></pre></td></tr></table></figure>
<h2 id="6、数据持久化"><a href="#6、数据持久化" class="headerlink" title="6、数据持久化"></a>6、数据持久化</h2><h3 id="1-Redis介绍"><a href="#1-Redis介绍" class="headerlink" title="1.Redis介绍"></a>1.Redis介绍</h3><p>Redis是REmote DIctionary Server的缩写，它是一个用ANSI C编写的高性能的key-value存储系统，与其他的key-value存储系统相比，Redis有以下一些特点（也是优点）：</p>
<ul>
<li>的Redis的读写性能极高，并且有丰富的特性（发布/订阅，事务，通知等）。</li>
<li>Redis的支持数据的持久化（RDB和AOF两种方式），可以将内存中的数据保存在磁盘中，重启的时候可以再次加载进行使用。</li>
<li>Redis的不仅仅支持简单的键 - 值类型的数据，同时还提供哈希，列表，设置，zset，hyperloglog，地理等数据类型。</li>
<li>Redis的支持主从复制（实现读写分析）以及哨兵模式（监控主是否宕机并调整配置）。</li>
</ul>
<p>Redis的在CentOS的中的安装配置已经上章文章中已经整理好了，如需查看自行跳转过去查看，<a href="https://github.com/coco369/knowledge/blob/master/sql/redis.md" target="_blank" rel="noopener">地址</a> .Redis有着非常丰富的数据类型，也有很多的命令来操作这些数据，具体的内容可以查看<a href="http://redisdoc.com/" target="_blank" rel="noopener">Redis的命令参考</a>，在这个网站上，除了Redis的的命令参考，还有Redis的的详细文档，其中包括了通知，事务，主从复制，持久化，哨兵，集群等内容。</p>
<p><strong>在Python程序中使用Redis</strong> </p>
<p>可以使用画中画安装Redis的模块.redis模块的核心是名为Redis的的类，该类的对象代表一个Redis的客户端，通过该客户端可以向Redis的服务器发送命令并获取执行的结果。我们在Redis的客户端中使用的命令基本上就是Redis的对象可以接收的消息。</p>
<p>首先：安装的Redis</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install redis</span><br></pre></td></tr></table></figure>
<p><strong>简单案例</strong> </p>
<p>该案例：用户在登录的时候，先验证传入的用户名和密码是否在Redis的中，是否能验证通过，如果不能验证通过的话，就去MySQL的数据库中验证，如果验证成功则同步的Redis中用户的信息。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line"></span><br><span class="line">import sys</span><br><span class="line"></span><br><span class="line">import pymysql</span><br><span class="line">import redis</span><br><span class="line"></span><br><span class="line"># 访问mysql数据库</span><br><span class="line">def con_mysql(sql):</span><br><span class="line">	db = pymysql.connect(</span><br><span class="line">		host=&apos;127.0.0.1&apos;,</span><br><span class="line">		user=&apos;root&apos;, </span><br><span class="line">		passwd=&apos;123456&apos;,</span><br><span class="line">		port=3306,</span><br><span class="line">		db=&apos;srs&apos;,</span><br><span class="line">		charset=&apos;utf8&apos;)</span><br><span class="line">	cursor = db.cursor()</span><br><span class="line">	cursor.execute(sql)</span><br><span class="line">	data = cursor.fetchall()</span><br><span class="line">	db.close()</span><br><span class="line">	return data</span><br><span class="line"></span><br><span class="line">def con_redis()</span><br><span class="line">    r = redis.Redis(host=&apos;47.92.164.198&apos;, port=6379)</span><br><span class="line">    return r</span><br><span class="line"></span><br><span class="line">def excute_redis(r, passwd, name):</span><br><span class="line">    r_name = r.hget(&apos;user&apos;,&apos;name&apos;)</span><br><span class="line">    r_passwd = r.hget(&apos;user&apos;, &apos;passwd&apos;)</span><br><span class="line">    r_name = r_name.decode(&apos;utf8&apos;)</span><br><span class="line">    r_passwd = r_passwd.decode(&apos;utf8&apos;)</span><br><span class="line">    if name == r_name and passwd == r_passwd:</span><br><span class="line">    	return True, &apos;登录成功&apos;</span><br><span class="line">    else:</span><br><span class="line">    	return False, &apos;登录失败&apos;</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">	# 获取传入的姓名和密码参数</span><br><span class="line">	if len(sys.argv):</span><br><span class="line">		name = sys.argv[1]</span><br><span class="line">		passwd = sys.argv[2]</span><br><span class="line">	    # 传入redis中，进行校验</span><br><span class="line">		r = con_redis()</span><br><span class="line">		result = excute_redis(r, passwd, name)</span><br><span class="line">		if not result[0]:</span><br><span class="line">			# 查询mysql数据库</span><br><span class="line">			sql = &apos;&apos;&apos;select * from stu where name=&quot;%s&quot; and passwd=&quot;%s&quot;&apos;&apos;&apos; % (name, passwd)</span><br><span class="line">			print(sql)</span><br><span class="line">			data = con_mysql(sql)</span><br><span class="line">			if data:</span><br><span class="line">				r = con_redis()</span><br><span class="line">			    # r = redis.Redis(host=&apos;47.92.164.198&apos;, port=6379)</span><br><span class="line">			    r.hset(&apos;user&apos;, &apos;name&apos;, name)</span><br><span class="line">			    r.hset(&apos;user&apos;, &apos;passwd&apos;, passwd)</span><br><span class="line">			    print(&apos;刷新redis，登录成功&apos;)</span><br><span class="line">			else:</span><br><span class="line">				print(&apos;用户名和密码错误&apos;)</span><br><span class="line">		else:</span><br><span class="line">			print(&apos;redis中数据正确，登录成功&apos;)</span><br></pre></td></tr></table></figure>
<h3 id="2-mongodb介绍"><a href="#2-mongodb介绍" class="headerlink" title="2. mongodb介绍"></a>2. mongodb介绍</h3><p>MongoDB的是2009年年问世的一个面向文档的数据库管理系统，由C ++语言编写，旨在为网络应用提供可扩展的高性能数据存储解决方案。虽然在划分类别的时候后，MongoDB的被认为是的NoSQL的产品，但是它更像一个介于关系数据库和非关系数据库之间的产品，在非关系数据库中它功能最丰富，最像关系数据库。</p>
<p>MongoDB的将数据存储为一个文档，一个文档由一系列的“键值对”组成，其文档类似于JSON对象，但是MongoDB的对JSON进行了二进制处理（能够更快的定位键和值），因此其文档的存储格式称为BSON。关于JSON和BSON的差别大家可以看看MongoDB官方网站的文章<a href="https://www.mongodb.com/json-and-bson" target="_blank" rel="noopener">“JSON和BSON”</a>。</p>
<p>目前，MongoDB中已经提供了对Windows中的MacOS，Linux和Solaris等多个平台的支持，而且也提供了多种开发语言的驱动程序，Python的当然是其中之一。</p>
<p><a href="https://github.com/coco369/knowledge/blob/master/sql/mongodb.md" target="_blank" rel="noopener">MongoDB中安装的配置</a>以及<a href="https://github.com/coco369/knowledge/blob/master/sql/mongodb%E8%AF%AD%E6%B3%95.md" target="_blank" rel="noopener">语法</a>操作都整理好了，可以自行前往回顾，熟悉语法。</p>
<p><strong>在python中操作mongodb</strong> </p>
<p>通过PIP安装pymongo来实现对MongoDB中的操作。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pymongo</span><br></pre></td></tr></table></figure>
<p>简单的访问蒙戈，并打印文档中的信息</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from pymongo import MongoClient</span><br><span class="line"></span><br><span class="line">client = MongoClient(&apos;mongodb://45.76.206.145:27017&apos;)</span><br><span class="line">db = client.spider</span><br><span class="line"></span><br><span class="line">for student in db.students.find():</span><br><span class="line">    print(student)</span><br></pre></td></tr></table></figure>
<h2 id="7、动态解析"><a href="#7、动态解析" class="headerlink" title="7、动态解析"></a>7、动态解析</h2><p>根据权威机构发布的全球互联网可访问性审计报告，全球约有四分之三的网站其内容或部分内容是通过JavaScript动态生成的，这就意味着在浏览器窗口中“查看网页源代码”时无法在HTML代码中找到这些内容，也就是说我们之前用的抓取数据的方式无法正常运转了。解决这样的问题基本上有两种方案：</p>
<blockquote>
<p>一是JavaScript逆向工程；</p>
</blockquote>
<blockquote>
<p>另一种是渲染JavaScript获得渲染后的内容。</p>
</blockquote>
<h3 id="JavaScript逆向工程"><a href="#JavaScript逆向工程" class="headerlink" title="JavaScript逆向工程"></a>JavaScript逆向工程</h3><p>我们以<a href="https://movie.douban.com/explore#!type=movie&amp;tag=%E7%83%AD%E9%97%A8&amp;sort=recommend&amp;page_limit=20&amp;page_start=0" target="_blank" rel="noopener">豆瓣电影</a>为例，说明什么是JavaScript逆向工程。其实所谓的JavaScript逆向工程就是找到通过Ajax请求动态获取数据的接口。</p>
<p>但是当我们在浏览器中通过右键菜单“显示网页源代码”的时候，居然惊奇的发现页面的HTML代码中连一个电影的名称都搜索不到。</p>
<p>那网页中的数据到底是怎么加载出来的呢，其实网页中的数据就是一个动态加载出来的。可以在浏览器的“开发人员工具”的“网络”中可以找到获取这些图片数据的网络API接口，如下图所示。</p>
<p>那么结论就很简单了，只要我们找到了这些网络API接口，那么就能通过这些接口获取到数据，当然实际开发的时候可能还要对这些接口的参数以及接口返回的数据进行分析，了解每个参数的意义以及返回的JSON数据的格式，这样才能在我们的爬虫中使用这些数据。</p>
<h3 id="selenium自动框架"><a href="#selenium自动框架" class="headerlink" title="selenium自动框架"></a>selenium自动框架</h3><p>使用自动化测试工具Selenium，它提供了浏览器自动化的API接口，这样就可以通过操控浏览器来获取动态内容。首先可以使用pip来安装Selenium。</p>
<h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install selenium</span><br></pre></td></tr></table></figure>
<h4 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h4><p>我们通过Selenium实现对Chrome浏览器的操控，如果要操控其他的浏览器，可以创对应的浏览器对象，例如Chrome、Firefox、Edge等，还有手机端的浏览器Android、BlackBerry等，另外无界面浏览器PhantomJS也同样支持。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from selenium import webdriver</span><br><span class="line"></span><br><span class="line">browser = webdriver.Firefox()</span><br><span class="line">browser = webdriver.Ie()</span><br><span class="line">browser = webdriver.Opera()</span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser = webdriver.PhantomJS()</span><br></pre></td></tr></table></figure>
<p>这样我们就完成了一个浏览器对象的初始化，接下来我们要做的就是调用browser对象，让其执行各个动作，就可以模拟浏览器操作了。</p>
<p>案例中我们使用Chrome浏览器，在模拟Chrome浏览器的时候，如果报如下的错误的话，说明你没有Chrome的驱动。接下来就是添加Chrome的驱动到我们的环境变频path中，或者在程序中指定Chrome驱动的位置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">selenium.common.exceptions.WebDriverException: Message: &apos;chromedriver&apos; executable needs to be in PATH. Please see https://sites.google.com/a/chromium.org/chromedriver/home</span><br></pre></td></tr></table></figure>
<p>驱动已经下载好了，保存地址在(spider/chromedriver_win32/)中</p>
<h4 id="访问url"><a href="#访问url" class="headerlink" title="访问url"></a>访问url</h4><p>可以用get()方法来请求一个网页，参数传入链接URL即可</p>
<h4 id="获取元素"><a href="#获取元素" class="headerlink" title="获取元素"></a>获取元素</h4><p>在浏览器中的操作，都可以通过selenium来完成，比如填充表单，模拟点击等等。那我们在进行这些操作的时候，首先需要知道我们要填充表单的位置在哪儿，模拟点击的按钮在哪儿。那怎么去获取这些信息呢。selenium中获取元素的方法有很多。</p>
<p><strong>获取单个元素</strong> </p>
<p>大概解释一下如下用法:</p>
<blockquote>
<p>find_element_by_name()是根据Name值获取</p>
</blockquote>
<blockquote>
<p>ind_element_by_id()是根据ID获取</p>
</blockquote>
<blockquote>
<p>find_element_by_xpath()是根据Xpath提取</p>
</blockquote>
<blockquote>
<p>find_element_by_css_selector(‘#xxx’)是根据id=xxx来获取</p>
</blockquote>
<blockquote>
<p>find_element()方法，它需要传入两个参数，一个是查找的方式By，另一个就是值，实际上它就是find_element_by_id()这种方法的通用函数版本。</p>
</blockquote>
<p>注意： from selenium.webdriver.common.by import By</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">find_element_by_id(id)</span><br><span class="line">就等价于find_element(By.ID, id)</span><br><span class="line"></span><br><span class="line">find_element_by_css_selector(&apos;#xxx&apos;)</span><br><span class="line">等价于find_elements(By.CSS_SELECTOR, &apos;.service-bd li&apos;)</span><br></pre></td></tr></table></figure>
<p><strong>获取多个元素</strong> </p>
<blockquote>
<p>find_elements_by_css_selector(‘#xxx li’)是根据id=xxx来获取下面的所有li的结果</p>
</blockquote>
<h4 id="查找淘宝导航条的所有条目"><a href="#查找淘宝导航条的所有条目" class="headerlink" title="查找淘宝导航条的所有条目"></a>查找淘宝导航条的所有条目</h4><p>案例代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">from selenium import webdriver</span><br><span class="line"></span><br><span class="line">chromedriver = &apos;C:\Program Files (x86)\Google\Chrome\Application\chromedriver&apos;</span><br><span class="line">browser = webdriver.Chrome(chromedriver)</span><br><span class="line">browser.get(&apos;https://www.taobao.com&apos;)</span><br><span class="line">lis = browser.find_elements_by_css_selector(&apos;.service-bd li a&apos;)</span><br><span class="line">for li in lis:</span><br><span class="line">    # 获取文本信息</span><br><span class="line">    print(li.text)</span><br><span class="line">    # 获取属性</span><br><span class="line">    print(li.get_attribute(&apos;href&apos;))</span><br><span class="line">browser.close()</span><br></pre></td></tr></table></figure>
<h4 id="延时等待"><a href="#延时等待" class="headerlink" title="延时等待"></a>延时等待</h4><p>在Selenium中，get()方法会在网页框架加载结束之后就结束执行，此时如果获取page_source可能并不是浏览器完全加载完成的页面，如果某些页面有额外的Ajax请求，我们在网页源代码中也不一定能成功获取到。所以这里我们需要延时等待一定时间确保元素已经加载出来。在这里等待的方式有两种，一种隐式等待，一种显式等待。</p>
<p>以访问知乎发现页面为案例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from selenium import webdriver</span><br><span class="line"></span><br><span class="line">chromedriver = &apos;C:\Program Files (x86)\Google\Chrome\Application\chromedriver&apos;</span><br><span class="line">browser = webdriver.Chrome(chromedriver)</span><br><span class="line"></span><br><span class="line"># 用implicitly_wait()方法实现了隐式等待。</span><br><span class="line">browser.implicitly_wait(10)</span><br><span class="line"></span><br><span class="line">browser.get(&apos;https://www.zhihu.com/explore&apos;)</span><br><span class="line">input = browser.find_element_by_class_name(&apos;zu-top-add-question&apos;)</span><br><span class="line">print(input)</span><br></pre></td></tr></table></figure>
<h4 id="前进后退"><a href="#前进后退" class="headerlink" title="前进后退"></a>前进后退</h4><p>使用back()方法可以后退，forward()方法可以前进</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">browser.back()</span><br><span class="line">time.sleep(1)</span><br><span class="line">browser.forward()</span><br></pre></td></tr></table></figure>
<h4 id="Cookies操作"><a href="#Cookies操作" class="headerlink" title="Cookies操作"></a>Cookies操作</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">from selenium import webdriver</span><br><span class="line"></span><br><span class="line">chromedriver = &apos;C:\Program Files (x86)\Google\Chrome\Application\chromedriver&apos;</span><br><span class="line">browser = webdriver.Chrome(chromedriver)</span><br><span class="line">browser.get(&apos;https://www.zhihu.com/explore&apos;)</span><br><span class="line"></span><br><span class="line"># 获取所有cookies信息</span><br><span class="line">print(browser.get_cookies())</span><br><span class="line"></span><br><span class="line"># 添加一个Cookie信息</span><br><span class="line">browser.add_cookie(&#123;&apos;name&apos;: &apos;王大帅&apos;, &apos;value&apos;: &apos;16&apos;&#125;)</span><br><span class="line">print(browser.get_cookies())</span><br><span class="line"># 删除所有的Cookies</span><br><span class="line">browser.delete_all_cookies()</span><br><span class="line">print(browser.get_cookies())</span><br><span class="line">browser.close()</span><br></pre></td></tr></table></figure>
<h4 id="切换窗口"><a href="#切换窗口" class="headerlink" title="切换窗口"></a>切换窗口</h4><p>以淘宝为例：</p>
<p>打开浏览器在主页中点击女装案例，再切换回主页再点击男装按钮，然后主页进行back()和froward()操作，最后退出整个浏览器quit()</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">import time</span><br><span class="line"></span><br><span class="line">from selenium import webdriver</span><br><span class="line"></span><br><span class="line">chromedriver = &apos;C:\Program Files (x86)\Google\Chrome\Application\chromedriver&apos;</span><br><span class="line">browser = webdriver.Chrome(chromedriver)</span><br><span class="line"></span><br><span class="line"># 隐式延迟</span><br><span class="line">browser.implicitly_wait(20)</span><br><span class="line"></span><br><span class="line">browser.get(&apos;https://www.taobao.com&apos;)</span><br><span class="line"></span><br><span class="line"># 获取主窗口</span><br><span class="line">taobao_handler = browser.current_window_handle</span><br><span class="line"></span><br><span class="line"># 点击导航栏女装按钮</span><br><span class="line">browser.find_element_by_xpath(&apos;/html/body/div[4]/div[1]/div[1]/div[1]/div/ul/li[1]/a[1]&apos;).click()</span><br><span class="line"></span><br><span class="line">time.sleep(3)</span><br><span class="line"></span><br><span class="line"># 获取当前窗口</span><br><span class="line">text_browser_handler_nv = browser.current_window_handle</span><br><span class="line"></span><br><span class="line"># 切换窗口继续点击男装案例</span><br><span class="line">browser.switch_to_window(taobao_handler)</span><br><span class="line"></span><br><span class="line"># # 点击导航栏男装按钮</span><br><span class="line">browser.find_element_by_xpath(&apos;/html/body/div[4]/div[1]/div[1]/div[1]/div/ul/li[1]/a[2]&apos;).click()</span><br><span class="line"></span><br><span class="line">time.sleep(3)</span><br><span class="line"></span><br><span class="line"># 后退</span><br><span class="line">browser.back()</span><br><span class="line"></span><br><span class="line">time.sleep(3)</span><br><span class="line"># 前进</span><br><span class="line">browser.forward()</span><br><span class="line"></span><br><span class="line"># 关闭浏览器</span><br><span class="line">browser.quit()</span><br></pre></td></tr></table></figure>
<p>获取一共启动了多少窗口：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">browser.window_handles</span><br></pre></td></tr></table></figure>
<h2 id="8、scrapy框架"><a href="#8、scrapy框架" class="headerlink" title="8、scrapy框架"></a>8、scrapy框架</h2><p>Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。</p>
<p>其最初是为了 页面抓取 (更确切来说, 网络抓取 )所设计的， 也可以应用在获取API所返回的数据或者通用的网络爬虫。</p>
<p>Scrapy 使用了 Twisted异步网络库来处理网络通讯。整体架构大致如下:</p>
<p><img src="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/spider_scrapy_zhujian.png" alt=""></p>
<p><a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/index.html" target="_blank" rel="noopener">中文官网</a></p>
<h3 id="初窥Scrapy"><a href="#初窥Scrapy" class="headerlink" title="初窥Scrapy"></a>初窥Scrapy</h3><h3 id="1-安装"><a href="#1-安装" class="headerlink" title="1. 安装"></a>1. 安装</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install Scrapy</span><br></pre></td></tr></table></figure>
<p>安装过程中会安装如下一些包，在之前的安装过程中，偶尔会出现Twisted失败的话，需要自己手动去安装。</p>
<p><img src="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/spider_scrapy_pip.png" alt=""></p>
<p>在此也先安装另外一个必备的包pywin32，如果不安装该包的话，在运行爬虫的时候可能会提示<strong>“ModuleNotFoundError: No module named ‘win32api’”</strong>，因为Python没有自带访问windows系统API的库的，需要下载第三方库。库的名称叫pywin32。可以去网站上下载，<a href="https://sourceforge.net/projects/pywin32/files/pywin32/Build%20221/" target="_blank" rel="noopener">下载地址</a></p>
<p><img src="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/scrapy_win32api.png" alt=""></p>
<p>按照自己电脑上的python版本，进行下载安装。安装的时候，先进入虚拟环境中，然后执行easy_install pywin32-221.win-amd64-py3.6.exe 命令即可将包安装在我们当前的虚拟环境中了。</p>
<p><img src="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/easyinstall_win32api.png" alt=""></p>
<h3 id="2-Scrapy组件"><a href="#2-Scrapy组件" class="headerlink" title="2. Scrapy组件"></a>2. Scrapy组件</h3><h4 id="1-引擎-Scrapy"><a href="#1-引擎-Scrapy" class="headerlink" title="1. 引擎(Scrapy)"></a>1. 引擎(Scrapy)</h4><p>用来处理整个系统的数据流处理, 触发事务(框架核心)</p>
<h4 id="2-调度器-Scheduler"><a href="#2-调度器-Scheduler" class="headerlink" title="2. 调度器(Scheduler)"></a>2. 调度器(Scheduler)</h4><p>用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列,<br>由它来决定下一个要抓取的网址是什么, 同时去除重复的网址</p>
<h4 id="3-下载器-Downloader"><a href="#3-下载器-Downloader" class="headerlink" title="3. 下载器(Downloader)"></a>3. 下载器(Downloader)</h4><p>用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)</p>
<h4 id="4-爬虫-Spiders"><a href="#4-爬虫-Spiders" class="headerlink" title="4. 爬虫(Spiders)"></a>4. 爬虫(Spiders)</h4><p>爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面</p>
<h4 id="5-项目管道-Pipeline"><a href="#5-项目管道-Pipeline" class="headerlink" title="5. 项目管道(Pipeline)"></a>5. 项目管道(Pipeline)</h4><p>负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，<br>将被发送到项目管道，并经过几个特定的次序处理数据。</p>
<h4 id="6-下载器中间件-Downloader-Middlewares"><a href="#6-下载器中间件-Downloader-Middlewares" class="headerlink" title="6. 下载器中间件(Downloader Middlewares)"></a>6. 下载器中间件(Downloader Middlewares)</h4><p>位于Scrapy引擎和下载器之间的框架，主要是处理Scrapy引擎与下载器之间的请求及响应。</p>
<h4 id="7-爬虫中间件-Spider-Middlewares"><a href="#7-爬虫中间件-Spider-Middlewares" class="headerlink" title="7. 爬虫中间件(Spider Middlewares)"></a>7. 爬虫中间件(Spider Middlewares)</h4><p>介于Scrapy引擎和爬虫之间的框架，主要工作是处理蜘蛛的响应输入和请求输出。</p>
<h4 id="8-调度中间件-Scheduler-Middewares"><a href="#8-调度中间件-Scheduler-Middewares" class="headerlink" title="8. 调度中间件(Scheduler Middewares)"></a>8. 调度中间件(Scheduler Middewares)</h4><p>介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应。</p>
<h3 id="3-处理流程"><a href="#3-处理流程" class="headerlink" title="3. 处理流程"></a>3. 处理流程</h3><p>Scrapy的整个数据处理流程由Scrapy引擎进行控制，通常的运转流程包括以下的步骤：</p>
<ol>
<li>引擎询问蜘蛛需要处理哪个网站，并让蜘蛛将第一个需要处理的URL交给它。</li>
<li>引擎让调度器将需要处理的URL放在队列中。</li>
<li>引擎从调度那获取接下来进行爬取的页面。</li>
<li>调度将下一个爬取的URL返回给引擎，引擎将它通过下载中间件发送到下载器。</li>
<li>当网页被下载器下载完成以后，响应内容通过下载中间件被发送到引擎；如果下载失败了，引擎会通知调度器记录这个URL，待会再重新下载。</li>
<li>引擎收到下载器的响应并将它通过蜘蛛中间件发送到蜘蛛进行处理。</li>
<li>蜘蛛处理响应并返回爬取到的数据条目，此外还要将需要跟进的新的URL发送给引擎。</li>
<li>引擎将抓取到的数据条目送入条目管道，把新的URL发送给调度器放入队列中。</li>
</ol>
<p>上述操作中的2-8步会一直重复直到调度器中没有需要请求的URL，爬虫停止工作。</p>
<h3 id="4-Scrapy项目"><a href="#4-Scrapy项目" class="headerlink" title="4. Scrapy项目"></a>4. Scrapy项目</h3><p>在创建项目开始，我们先确认一下之前安装的scrapy能否正常运行，如下情况即安装成功：</p>
<p><img src="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/scrapy_run_not_error.png" alt=""></p>
<h4 id="4-1-创建项目"><a href="#4-1-创建项目" class="headerlink" title="4.1 创建项目"></a>4.1 创建项目</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject dbspider</span><br></pre></td></tr></table></figure>
<p>创建成功以后，在我们的文件夹中会发现一个dbspider的目录，这个项目文件就是我们的爬虫项目了。可以先看看它的构成，接下来详细讲解一下每一个文件代表的意思。</p>
<p><img src="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/spider_scrapy_project.png" alt=""></p>
<h4 id="4-2-文件解释"><a href="#4-2-文件解释" class="headerlink" title="4.2 文件解释"></a>4.2 文件解释</h4><p>文件说明：</p>
<p><strong>scrapy.cfg</strong>:项目的配置信息，主要为Scrapy命令行工具提供一个基础的配置信息。（真正爬虫相关的配置信息在settings.py文件中）</p>
<p><strong>items.py</strong>:设置数据存储模板，用于结构化数据，如：Django的Model</p>
<p><strong>pipelines</strong>:数据处理行为，如：一般结构化的数据持久化</p>
<p><strong>settings.py</strong>:配置文件，如：递归的层数、并发数，延迟下载等</p>
<p><strong>spiders</strong>:爬虫目录，如：创建文件，编写爬虫规则。</p>
<p>在spiders文件中创建爬虫的时候，一般以爬取的网站的域名为爬虫的名称</p>
<h3 id="5-编写爬虫"><a href="#5-编写爬虫" class="headerlink" title="5. 编写爬虫"></a>5. 编写爬虫</h3><p>爬取起点中文网的网页源码,爬取小说分类名称以及url</p>
<p>案例代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line">from scrapy.selector import Selector</span><br></pre></td></tr></table></figure>
<p> class QiDianSpider(scrapy.spiders.Spider): name = “qidian” start_urls = [ “<a href="https://www.qidian.com/" target="_blank" rel="noopener">https://www.qidian.com/</a>“, ]</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def parse(self, response):</span><br><span class="line">    # 爬取时请求的url</span><br><span class="line">    current_url = response.url</span><br><span class="line"></span><br><span class="line">    # 返回的html</span><br><span class="line">    body = response.body</span><br><span class="line"></span><br><span class="line">    # 返回的html unicode编码</span><br><span class="line">    unicode_body = response.body_as_unicode()</span><br><span class="line">    res = Selector(response)</span><br><span class="line"></span><br><span class="line">    # 获取小说的分类信息</span><br><span class="line">    xiaoshuo_type = res.xpath(&apos;//*[@id=&quot;pin-nav&quot;]/div/div[1]/ul/li/a/text()&apos;).extract()</span><br><span class="line"></span><br><span class="line">    xiaoshuo_href = res.xpath(&apos;//*[@id=&quot;pin-nav&quot;]/div/div[1]/ul/li/a/@href&apos;).extract()</span><br><span class="line"></span><br><span class="line">    print(xiaoshuo_type, xiaoshuo_href)</span><br></pre></td></tr></table></figure>
<h4 id="5-1-运行命令："><a href="#5-1-运行命令：" class="headerlink" title="5.1 运行命令："></a>5.1 运行命令：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl qidian</span><br></pre></td></tr></table></figure>
<p>启动命令中 ‘qidian’参数为我们定义爬虫中的name属性的值</p>
<p>执行流程：</p>
<blockquote>
<p>name: spider对应不同的name</p>
</blockquote>
<blockquote>
<p>start_urls:是spider抓取网页的起始点，可以包括多个url。</p>
</blockquote>
<blockquote>
<p>parse()：spider抓到一个网页以后默认调用的callback，避免使用这个名字来定义自己的方法。当spider拿到url的内容以后，会调用parse方法，并且传递一个response参数给它，response包含了抓到的网页的内容，在parse方法里，你可以从抓到的网页里面解析数据。</p>
</blockquote>
<p>运行结果：</p>
<p><img src="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/scrapy_qidian_type.png" alt=""></p>
<h2 id="9、分布式爬虫"><a href="#9、分布式爬虫" class="headerlink" title="9、分布式爬虫"></a>9、分布式爬虫</h2><p>说到分布式系统的时候，要和集中式系统进行对比的学习，下面就先介绍下集中式系统，对比它们的优缺点进行学习。</p>
<h4 id="集中式系统"><a href="#集中式系统" class="headerlink" title="集中式系统"></a>集中式系统</h4><p>集中式系统：</p>
<p>集中式系统中整个项目就是一个独立的应用，整个应用也就是整个项目，所有的业务逻辑功能都在一个应用里面。如果遇到并发的瓶颈的时候，就多增加几台服务器来部署项目，以此来解决并发分问题。在nginx中进行负载均衡即可。</p>
<p>缺点：</p>
<blockquote>
<p>a) 不易于扩展</p>
</blockquote>
<blockquote>
<p>b) 如果发现你的项目代码中有bug的话，那么你的所有的服务器中的项目代码都是有问题的，这时候要更新这个bug的时候，就需要同时更新所有的服务器了。</p>
</blockquote>
<p>优点：</p>
<blockquote>
<p>维护方便</p>
</blockquote>
<h4 id="分布式系统"><a href="#分布式系统" class="headerlink" title="分布式系统"></a>分布式系统</h4><p>分布式系统:</p>
<p>分布式系统中，我们的整个项目可以拆分成很多业务块，每一个业务块单独进行集群的部署。这样就将整个项目分开了，在进行拓展的时候，系统是很容易横向拓展的。在并发的时候，也很好的将用户的并发量提上去。</p>
<p>缺点：</p>
<blockquote>
<p>a) 项目拆分的过于复杂，给运维带来了很高的维护成本</p>
</blockquote>
<blockquote>
<p>b) 数据的一致性，分布式事务，分布式锁等问题不能得到很好的解决</p>
</blockquote>
<p>优点：</p>
<blockquote>
<p>a) 一个业务模块崩了，并不影响其他的业务</p>
</blockquote>
<blockquote>
<p>b) 利于扩展</p>
</blockquote>
<blockquote>
<p>c) 在上线某个新功能的时候，只需要新增对应的分布式的节点即可，测试也只需要测试该业务功能即可。很好的避免了测试在上线之前需要将整个系统进行全方面的测试</p>
</blockquote>
<h3 id="1-scrapy的分布式原理"><a href="#1-scrapy的分布式原理" class="headerlink" title="1. scrapy的分布式原理"></a>1. scrapy的分布式原理</h3><p>我们还是先回顾下scrapy的运行原理的构造图:</p>
<p><img src="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/spider_scrapy_zhujian.png" alt=""></p>
<p>该图很好的阐释了在不是scrapy的服务器中的运行结构图，在维护爬取的url队列的时候，使用scheduler进行调度的。那么如果要修改为分布式的scrapy爬虫的话，其实就是将爬取的队列进行共享，多台部署了scrapy爬虫的服务器共享该爬取队列。</p>
<h3 id="2-分布式架构："><a href="#2-分布式架构：" class="headerlink" title="2. 分布式架构："></a>2. 分布式架构：</h3><p><img src="https://raw.githubusercontent.com/shuaixiaohao/shuaixiaohao.github.io/master/blog-image/%E7%88%AC%E8%99%AB/scrapy_redis_tu.png" alt=""></p>
<p>master-主机：维护爬虫队列。</p>
<p>slave-从机：数据爬取，数据处理，数据存储。</p>
<h3 id="3-搭建分布式爬虫"><a href="#3-搭建分布式爬虫" class="headerlink" title="3. 搭建分布式爬虫"></a>3. 搭建分布式爬虫</h3><p>我们使用scrapy_redis进行分布式爬虫的搭建。</p>
<p>scrapy_redis是scrapy框架下的一个插件，通过重构调度器来使我们的爬虫运行的更快</p>
<h4 id="3-1-安装"><a href="#3-1-安装" class="headerlink" title="3.1 安装"></a>3.1 安装</h4><p>安装scrapy_redis：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install scrapy_redis</span><br></pre></td></tr></table></figure>
<p>安装redis：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># redis可以仅在master主机上安装</span><br><span class="line"></span><br><span class="line">pip install redis</span><br></pre></td></tr></table></figure>
<p>安装数据存储数据库，采用mongodb <a href="https://github.com/coco369/knowledge/blob/master/sql/mongodb.md" target="_blank" rel="noopener">见: 安装配置地址</a></p>
<h4 id="3-2-redis"><a href="#3-2-redis" class="headerlink" title="3.2 redis"></a>3.2 redis</h4><p>在维护爬虫队列的时候，很多爬虫项目同时读取队列中的信息，就造成了可能读数据重复了，比如同时读取同一个url。为了避免这种情况，我们建议使用redis去维护队列。而且<strong>redis的集合中的元素还不是重复的</strong>，可以很好的利用这一点，进行url爬取地址的存储</p>
<h4 id="3-3-分布式爬虫改造"><a href="#3-3-分布式爬虫改造" class="headerlink" title="3.3 分布式爬虫改造"></a>3.3 分布式爬虫改造</h4><h5 id="3-3-1-master"><a href="#3-3-1-master" class="headerlink" title="3.3.1 master"></a>3.3.1 master</h5><p>master主机改造： 在master主机上安装redis并启动，最好设置密码</p>
<h5 id="spiders文件中定义的爬虫py文件修改如下："><a href="#spiders文件中定义的爬虫py文件修改如下：" class="headerlink" title="spiders文件中定义的爬虫py文件修改如下："></a>spiders文件中定义的爬虫py文件修改如下：</h5><p>如下爬虫实现的功能是拿到需要爬取的成都各大区县的二手房页面url地址，包括分页的地址。并将数据存储到redis中</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">import json</span><br><span class="line"></span><br><span class="line">from scrapy import Request</span><br><span class="line">from scrapy.spiders import Spider</span><br><span class="line">from scrapy.selector import Selector</span><br><span class="line"></span><br><span class="line">from lianjiaspider.items import LianjiaspiderItem, MasterItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class LianJiaSpider(Spider):</span><br><span class="line"></span><br><span class="line">    name = &apos;lianjia&apos;</span><br><span class="line">    # allowed_domains = [&apos;lianjia.com&apos;]</span><br><span class="line">    domains_url = &apos;https://cd.lianjia.com&apos;</span><br><span class="line">    start_linjia_url = &apos;https://cd.lianjia.com/ershoufang&apos;</span><br><span class="line"></span><br><span class="line">    def start_requests(self):</span><br><span class="line">        yield Request(self.start_linjia_url)</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line"></span><br><span class="line">        sel = Selector(response)</span><br><span class="line">        ershoufang_aera = sel.xpath(&apos;//div[@data-role=&quot;ershoufang&quot;]&apos;)</span><br><span class="line">        area_info = ershoufang_aera.xpath(&apos;./div/a&apos;)</span><br><span class="line"></span><br><span class="line">        for area in area_info:</span><br><span class="line">            area_href = area.xpath(&apos;./@href&apos;).extract()[0]</span><br><span class="line">            area_name = area.xpath(&apos;./text()&apos;).extract()[0]</span><br><span class="line"></span><br><span class="line">            yield Request(self.domains_url + area_href,</span><br><span class="line">                          callback=self.parse_house_info,</span><br><span class="line">                          meta=&#123;&apos;name&apos;: area_name, &apos;href&apos;: area_href&#125;)</span><br><span class="line"></span><br><span class="line">    def parse_house_info(self, response):</span><br><span class="line">        sel = Selector(response)</span><br><span class="line">        page_box = sel.xpath(&apos;//div[@class=&quot;page-box house-lst-page-box&quot;]/@page-data&apos;).extract()</span><br><span class="line">        total_page = json.loads(page_box[0]).get(&apos;totalPage&apos;)</span><br><span class="line"></span><br><span class="line">        for i in range(1, int(total_page)+1):</span><br><span class="line">            item = MasterItem()</span><br><span class="line">            item[&apos;url&apos;] = self.domains_url + response.meta.get(&apos;href&apos;) + &apos;pg&apos; + str(i)</span><br><span class="line">            yield item</span><br></pre></td></tr></table></figure>
<h5 id="定义Item"><a href="#定义Item" class="headerlink" title="定义Item"></a>定义Item</h5><p>接收一个地址url参数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">class MasterItem(scrapy.Item):</span><br><span class="line">    url = scrapy.Field()</span><br></pre></td></tr></table></figure>
<h5 id="新增redis存储中间件"><a href="#新增redis存储中间件" class="headerlink" title="新增redis存储中间件"></a>新增redis存储中间件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">class MasterPipeline(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">		# 链接redis</span><br><span class="line">        self.r = redis.Redis(host=&apos;127.0.0.1&apos;, port=6379)</span><br><span class="line"></span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">		# 向redis中插入需要爬取的链接地址</span><br><span class="line">        self.r.lpush(&apos;lianjia:start_urls&apos;, item[&apos;url&apos;])</span><br></pre></td></tr></table></figure>
<h5 id="3-3-2-slave改造："><a href="#3-3-2-slave改造：" class="headerlink" title="3.3.2 slave改造："></a>3.3.2 slave改造：</h5><p>slave从机改造：slave从机访问redis，直接去访问master主机上的redis的地址，以及端口密码等信息</p>
<h5 id="spiders爬虫文件改造"><a href="#spiders爬虫文件改造" class="headerlink" title="spiders爬虫文件改造"></a>spiders爬虫文件改造</h5><p>继承改为继承Redisspider</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from scrapy_redis.spiders import RedisSpider</span><br></pre></td></tr></table></figure>
<p>具体代码优化如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">from scrapy_redis.spiders import RedisSpider</span><br><span class="line">from scrapy.selector import Selector</span><br><span class="line"></span><br><span class="line">from lianjiaspider.items import LianjiaspiderItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class LianJiaSpider(RedisSpider):</span><br><span class="line"></span><br><span class="line">    name = &apos;lianjia&apos;</span><br><span class="line"></span><br><span class="line">	# 指定访问redis的爬取urls的队列</span><br><span class="line">    redis_key = &apos;lianjia:start_urls&apos;</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line"></span><br><span class="line">        sel = Selector(response)</span><br><span class="line">        lis = sel.xpath(&apos;//html/body/div[4]/div[1]/ul/li[@class=&quot;clear&quot;]&apos;)</span><br><span class="line">        for li in lis:</span><br><span class="line"></span><br><span class="line">            item = LianjiaspiderItem()</span><br><span class="line">            item[&apos;house_code&apos;] = li.xpath(&apos;./a/@data-housecode&apos;).extract()[0]</span><br><span class="line">            if li.xpath(&apos;./a/img/@src&apos;).extract():</span><br><span class="line">                item[&apos;img_src&apos;] = li.xpath(&apos;./a/img/@src&apos;).extract()[0]</span><br><span class="line">            if li.xpath(&apos;./div/div/a/text()&apos;).extract():</span><br><span class="line">                item[&apos;title&apos;] = li.xpath(&apos;./div/div/a/text()&apos;).extract()[0]</span><br><span class="line">            item[&apos;address&apos;] = li.xpath(&apos;./div/div[2]/div/a/text()&apos;).extract()</span><br><span class="line">            item[&apos;info&apos;] = li.xpath(&apos;./div/div[2]/div/text()&apos;).extract()</span><br><span class="line">            item[&apos;flood&apos;] = li.xpath(&apos;./div/div[3]/div/text()&apos;).extract()</span><br><span class="line">            item[&apos;tag&apos;] = li.xpath(&apos;.//div[@class=&quot;tag&quot;]/span/text()&apos;).extract()</span><br><span class="line">            item[&apos;type&apos;] = &apos;ershoufang&apos;</span><br><span class="line">            item[&apos;city&apos;] = &apos;成都&apos;</span><br><span class="line"></span><br><span class="line">            yield item</span><br><span class="line"></span><br><span class="line">    def split_house_info(self, info):</span><br><span class="line">        return [i.strip() for i in info.split(&apos;|&apos;)[1:]]</span><br></pre></td></tr></table></figure>
<h5 id="settings-py配置改造"><a href="#settings-py配置改造" class="headerlink" title="settings.py配置改造"></a>settings.py配置改造</h5><p>新增如下的配置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># scrapy-redis</span><br><span class="line">REDIS_URL = &apos;redis://:yzd@127.0.0.1:6379&apos;  # for master</span><br><span class="line"># REDIS_URL = &apos;redis://:yzd@10.140.0.2:6379&apos;  # for slave (master&apos;s ip)</span><br><span class="line"></span><br><span class="line"># SCHEDULER 是任务分发与调度，把所有的爬虫开始的请求都放在redis里面，所有爬虫都去redis里面读取请求。</span><br><span class="line">SCHEDULER = &quot;scrapy_redis.scheduler.Scheduler&quot;</span><br><span class="line"></span><br><span class="line"># 如果这一项设为True，那么在Redis中的URL队列不会被清理掉，但是在分布式爬虫共享URL时，要防止重复爬取。如果设为False，那么每一次读取URL后都会将其删掉，但弊端是爬虫暂停后重新启动，他会重新开始爬取。 </span><br><span class="line">SCHEDULER_PERSIST = True</span><br><span class="line"></span><br><span class="line"># REDIS_START_URLS_AS_SET指的是使用redis里面的set类型（简单完成去重），如果你没有设置，默认会选用list。</span><br><span class="line">REDIS_START_URLS_AS_SET = True</span><br><span class="line"></span><br><span class="line"># DUPEFILTER_CLASS 是去重队列，负责所有请求的去重</span><br><span class="line">DUPEFILTER_CLASS = &quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;</span><br><span class="line"></span><br><span class="line"># 爬虫的请求调度算法，有三种可供选择</span><br><span class="line"># scrapy_redis.queue.SpiderQueue：队列。先入先出队列，先放入Redis的请求优先爬取；</span><br><span class="line"># scrapy_redis.queue.SpiderStack：栈。后放入Redis的请求会优先爬取；</span><br><span class="line"># scrapy_redis.queue.SpiderPriorityQueue：优先级队列。根据优先级算法计算哪个先爬哪个后爬</span><br><span class="line">SCHEDULER_QUEUE_CLASS = &quot;scrapy_redis.queue.SpiderQueue&quot;</span><br><span class="line"></span><br><span class="line"># 设置链接redis的配置，或者如下分别设置端口和IP地址</span><br><span class="line">REDIS_URL = &apos;redis://127.0.0.1:6379&apos;</span><br><span class="line"></span><br><span class="line"># 分布式爬虫设置Ip端口</span><br><span class="line">REDIS_HOST = &apos;127.0.0.1&apos;</span><br><span class="line">REDIS_PORT = 6379</span><br></pre></td></tr></table></figure>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div></div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.jpg" alt="冯  昊 微信支付"/>
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpg" alt="冯  昊 支付宝"/>
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/爬虫/" rel="tag"># 爬虫</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/11/29/HTML学习/" rel="next" title="HTML">
                <i class="fa fa-chevron-left"></i> HTML
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/12/15/Flask学习/" rel="prev" title="Flask">
                Flask <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
 	<div id="gitalk-container"></div>






  




        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/fenghao.png"
                alt="冯  昊" />
            
              <p class="site-author-name" itemprop="name">冯  昊</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/shuaixiaohao" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:fenghao1994@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://weibo.com/realfenghao" target="_blank" title="微博">
                      
                        <i class="fa fa-fw fa-weibo"></i>微博</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.zhihu.com/people/shuaixiaohao" target="_blank" title="知乎">
                      
                        <i class="fa fa-fw fa-zhihu"></i>知乎</a>
                  </span>
                
            </div>
          

          <!--  音乐  -->
          
          <audio src="/music/月牙湾.mp3" style="max-height :100%; max-width: 100%; display: block; margin-left: auto; margin-right: auto;" controls="controls" loop="loop" preload="meta">Your browser does not support the audio tag.</audio>



          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                友情链接
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.zcool.com.cn/work/ZMzI3ODMzODQ=.html" title="何杨作品集" target="_blank">何杨作品集</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、网络爬虫和相关工具"><span class="nav-text">1、网络爬虫和相关工具</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-网络爬虫"><span class="nav-text">1.1 网络爬虫</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#爬虫的应用领域"><span class="nav-text">爬虫的应用领域</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2合法性和背景调研"><span class="nav-text">1.2合法性和背景调研</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#爬虫合法性探讨"><span class="nav-text">爬虫合法性探讨</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Robots-txt文件"><span class="nav-text">Robots.txt文件</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-相关工具介绍"><span class="nav-text">1.3 相关工具介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#HTTP协议"><span class="nav-text">HTTP协议</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#相关工具"><span class="nav-text">相关工具</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#一个简单的爬虫"><span class="nav-text">一个简单的爬虫</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#爬虫注意事项"><span class="nav-text">爬虫注意事项</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、前言"><span class="nav-text">2、前言</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-数据分析"><span class="nav-text">2.1 数据分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-请求头分析"><span class="nav-text">2.2 请求头分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-urllib的使用"><span class="nav-text">2.3 urllib的使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-requests"><span class="nav-text">2.4 requests</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-ssl认证"><span class="nav-text">2.5 ssl认证</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、数据采集和解析"><span class="nav-text">3、数据采集和解析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#正则表达式"><span class="nav-text">正则表达式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#XPath语法与Lxml库"><span class="nav-text">XPath语法与Lxml库</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BeautifulSoup"><span class="nav-text">BeautifulSoup</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PyQuery"><span class="nav-text">PyQuery</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4、并发、并行、同步、异步线程、进程"><span class="nav-text">4、并发、并行、同步、异步线程、进程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-同步和异步、阻塞和非阻塞"><span class="nav-text">4.1 同步和异步、阻塞和非阻塞</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#同步和异步"><span class="nav-text">同步和异步</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#阻塞和非阻塞"><span class="nav-text">阻塞和非阻塞</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#同步和阻塞的区别"><span class="nav-text">同步和阻塞的区别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#并发和并行"><span class="nav-text">并发和并行</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-进程"><span class="nav-text">4.2 进程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#杀掉进程"><span class="nav-text">杀掉进程</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-线程"><span class="nav-text">4.3 线程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#多线程"><span class="nav-text">多线程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#守护线程"><span class="nav-text">守护线程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#线程启动"><span class="nav-text">线程启动</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-线程锁"><span class="nav-text">4.4 线程锁</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#锁的概念"><span class="nav-text">锁的概念</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5、异步I-O"><span class="nav-text">5、异步I/O</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#协程的概念"><span class="nav-text">协程的概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#aiohttp"><span class="nav-text">aiohttp</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6、数据持久化"><span class="nav-text">6、数据持久化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Redis介绍"><span class="nav-text">1.Redis介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-mongodb介绍"><span class="nav-text">2. mongodb介绍</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7、动态解析"><span class="nav-text">7、动态解析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#JavaScript逆向工程"><span class="nav-text">JavaScript逆向工程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#selenium自动框架"><span class="nav-text">selenium自动框架</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#安装"><span class="nav-text">安装</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#使用"><span class="nav-text">使用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#访问url"><span class="nav-text">访问url</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#获取元素"><span class="nav-text">获取元素</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#查找淘宝导航条的所有条目"><span class="nav-text">查找淘宝导航条的所有条目</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#延时等待"><span class="nav-text">延时等待</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#前进后退"><span class="nav-text">前进后退</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Cookies操作"><span class="nav-text">Cookies操作</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#切换窗口"><span class="nav-text">切换窗口</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8、scrapy框架"><span class="nav-text">8、scrapy框架</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#初窥Scrapy"><span class="nav-text">初窥Scrapy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-安装"><span class="nav-text">1. 安装</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Scrapy组件"><span class="nav-text">2. Scrapy组件</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-引擎-Scrapy"><span class="nav-text">1. 引擎(Scrapy)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-调度器-Scheduler"><span class="nav-text">2. 调度器(Scheduler)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-下载器-Downloader"><span class="nav-text">3. 下载器(Downloader)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-爬虫-Spiders"><span class="nav-text">4. 爬虫(Spiders)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-项目管道-Pipeline"><span class="nav-text">5. 项目管道(Pipeline)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-下载器中间件-Downloader-Middlewares"><span class="nav-text">6. 下载器中间件(Downloader Middlewares)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-爬虫中间件-Spider-Middlewares"><span class="nav-text">7. 爬虫中间件(Spider Middlewares)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-调度中间件-Scheduler-Middewares"><span class="nav-text">8. 调度中间件(Scheduler Middewares)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-处理流程"><span class="nav-text">3. 处理流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Scrapy项目"><span class="nav-text">4. Scrapy项目</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-创建项目"><span class="nav-text">4.1 创建项目</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-文件解释"><span class="nav-text">4.2 文件解释</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-编写爬虫"><span class="nav-text">5. 编写爬虫</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-运行命令："><span class="nav-text">5.1 运行命令：</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9、分布式爬虫"><span class="nav-text">9、分布式爬虫</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#集中式系统"><span class="nav-text">集中式系统</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#分布式系统"><span class="nav-text">分布式系统</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-scrapy的分布式原理"><span class="nav-text">1. scrapy的分布式原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-分布式架构："><span class="nav-text">2. 分布式架构：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-搭建分布式爬虫"><span class="nav-text">3. 搭建分布式爬虫</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-安装"><span class="nav-text">3.1 安装</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-redis"><span class="nav-text">3.2 redis</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-分布式爬虫改造"><span class="nav-text">3.3 分布式爬虫改造</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#3-3-1-master"><span class="nav-text">3.3.1 master</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#spiders文件中定义的爬虫py文件修改如下："><span class="nav-text">spiders文件中定义的爬虫py文件修改如下：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#定义Item"><span class="nav-text">定义Item</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#新增redis存储中间件"><span class="nav-text">新增redis存储中间件</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-3-2-slave改造："><span class="nav-text">3.3.2 slave改造：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#spiders爬虫文件改造"><span class="nav-text">spiders爬虫文件改造</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#settings-py配置改造"><span class="nav-text">settings.py配置改造</span></a></li></ol></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        ﻿<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<div class="copyright">&copy; 2017 &mdash; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">冯  昊</span>

  
</div>










<div class="powered-by">
<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
  访客数:<span id="busuanzi_value_site_uv"></span>
</span>
</div>
|
<span id="busuanzi_container_page_pv">
  总阅读量<span id="busuanzi_value_page_pv"></span>次
</span>
        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  















  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
  <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
   <script type="text/javascript">
        var gitalk = new Gitalk({
          clientID: '02e98d4fe2acfa75f722',
          clientSecret: '4fe3f0b2f74b9ff99587c5f3c1e1717db5222be7',
          repo: 'shuaixiaohao.github.io',
          owner: 'shuaixiaohao',
          admin: ['shuaixiaohao'],
          id: location.pathname,
          distractionFreeMode: 'true'
        })
        gitalk.render('gitalk-container')           
       </script>

  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("zTDhPq9nvImrdSY5TaOaFrXU-gzGzoHsz", "UGcsGNXzQTAhlTTENHKpM2kG");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  


  

  

</body>
</html>
